# Suba datos en BD

## 쯇or qu칠 mi organizaci칩n deber칤a subir datos a BD?

- **Capacidad de cruzar sus bases con datos de diferentes
  organizaciones** de forma simple y f치cil. Ya hay cientos de conjuntos
  de datos p칰blicos de las mayores organizaciones de Brasil y del mundo presentes
  en nuestro *data lake*.

- **Compromiso con la transparencia, calidad de los datos y
  desarrollo de mejores investigaciones, an치lisis y soluciones** para la
  sociedad. No solo democratizamos el acceso a datos abiertos, sino tambi칠n datos
  de calidad. Tenemos un equipo especializado que revisa y garantiza la calidad de los
  datos a침adidos al *data lake*.

- **Participaci칩n en una comunidad que crece cada vez m치s**: miles
  de periodistas, investigadores(as), desarrolladores(as), ya utilizan y
  siguen la Base de los Datos.
  <!-- TODO: Colocar aqu칤 el enlace de nuestro panel de audiencia cuando est칠 listo :) -->

## Paso a paso para subir datos

쯈uieres subir datos a BD y ayudarnos a construir este repositorio?
춰*Maravilloso!* Organizamos todo lo que necesitas en el manual a continuaci칩n en 8 pasos

Para facilitar la explicaci칩n, seguiremos un ejemplo ya listo con datos de [RAIS](https://basedosdados.org/es/dataset/3e7c4d58-96ba-448e-b053-d385a829ef00).

!!! Tip "Puedes navegar por las etapas en el men칰 de la izquierda."
    춰Sugerimos encarecidamente que te unas a nuestro [canal en
    Discord](https://discord.gg/huKWpsVYx4) para resolver dudas e
    interactuar con el equipo y otros(as) colaboradores(as)! 游땔

### Antes de empezar

Algunos conocimientos son necesarios para realizar este proceso:

- **Python, R, SQL y/o Stata**: para crear los c칩digos de captura y limpieza de los datos.
- **L칤nea de comandos**: para configurar tu ambiente local
  y conexi칩n con Google Cloud.
- **Github**: para subir tu c칩digo para revisi칩n de
  nuestro equipo.

!!! Tip "쯅o tienes alguna de estas habilidades, pero quieres colaborar?"
    Tenemos un equipo de datos que puede ayudarte, solo 칰nete a [nuestro
    Discord](https://discord.gg/huKWpsVYx4) y env칤a un mensaje en #quiero-contribuir.

### 쮺칩mo funciona el proceso?

- [1. Elegir la base y entender m치s de los datos](#1-elegir-la-base-y-entender-mas-de-los-datos) - primero necesitamos conocer lo que estamos tratando.
- [2. Descargar nuestra carpeta template](#2-descargar-nuestra-carpeta-template) - es hora de estructurar el trabajo a realizar
- [3. Completar las tablas de arquitectura](#3-completar-las-tablas-de-arquitectura) - es primordial definir la estructura de los datos antes de iniciar el tratamiento
- [4. Escribir c칩digo de captura y limpieza de datos](#4-escribir-codigo-de-captura-y-limpieza-de-datos) - 춰hora de poner manos a la obra!
- [5. (Si es necesario) Organizar archivos auxiliares](#5-si-es-necesario-organizar-archivos-auxiliares) - porque hasta los datos necesitan gu칤as
- [6. (Si es necesario) Crear tabla diccionario](#6-si-es-necesario-crear-tabla-diccionario) - momento de armar los diccionarios
- [7. Subir todo a Google Cloud](#7-subir-todo-a-google-cloud) - despu칠s de todo, es all칤 donde est치n los datos de BD
- [8. Enviar todo para revisi칩n](#8-enviar-todo-para-revision) - 춰una mirada de nuestro equipo para garantizar que todo est치 listo para producci칩n!

### 1. Elegir la base y entender m치s de los datos

Mantenemos la lista de conjuntos para voluntarios en nuestro [Github](https://github.com/orgs/basedosdados/projects/17/views/10). Para empezar a subir una base de tu inter칠s, solo abre un [nuevo issue](https://github.com/basedosdados/queries-basedosdados/issues/new?assignees=&labels=&projects=&template=issue--novos-dados.md&title=%5Bdados%5D+%3Cdataset_id%3E) de datos. Si tu base (conjunto) ya est치 listada, solo marca tu usuario de Github como `assignee`

Tu primer trabajo es completar la informaci칩n en el issue. Esta informaci칩n te ayudar치 a entender mejor los datos y ser치 muy 칰til para el tratamiento y el llenado de metadatos.

Cuando finalices esta etapa, llama a alguien del equipo de datos para que la informaci칩n que has mapeado sobre el conjunto ya entre en nuestro sitio!

### 2. Descargar nuestra carpeta template

[Descarga aqu칤 la carpeta
_template_](https://drive.google.com/drive/folders/1xXXon0vdjSKr8RCNcymRdOKgq64iqfS5?usp=sharing)
 y ren칩mbrala como `<dataset_id>` (definido en el issue del [paso 1](#-1-Elegir-la-base-y-entender-mas-de-los-datos)). Esta carpeta template facilita y organiza todos los
pasos de aqu칤 en adelante. Su
estructura es la siguiente:

- `<dataset_id>/`
    - `code/`: C칩digos necesarios para **captura** y **limpieza** de los datos
    ([veremos m치s en el paso
    4](#4-escribir-codigo-de-captura-y-limpieza-de-datos)).
    - `input/`: Contiene todos los archivos con datos originales, exactamente
    como se descargaron de la fuente primaria. ([veremos m치s en el paso
    4](#4-escribir-codigo-de-captura-y-limpieza-de-datos)).
    - `output/`: Archivos finales, ya en el formato listo para subir a BD ([veremos m치s en el paso
    4](#4-escribir-codigo-de-captura-y-limpieza-de-datos)).
    - `tmp/`: Cualquier archivo temporal creado por el c칩digo en `/code` en el proceso de limpieza y tratamiento ([veremos m치s en el paso
    4](#4-escribir-codigo-de-captura-y-limpieza-de-datos)).
    - `extra/`
        - `architecture/`: Tablas de arquitectura ([veremos m치s en el paso 3](#3-completar-las-tablas-de-arquitectura)).
        - `auxiliary_files/`: Archivos auxiliares a los datos ([veremos m치s en el paso 5](#5-si-es-necesario-organizar-archivos-auxiliares)).
        - `dicionario.csv`: Tabla diccionario de todo el conjunto de datos ([veremos m치s en el paso 6](#6-si-es-necesario-crear-tabla-diccionario)).

!!! info "Solo la carpeta `code` ser치 commitada para tu proyecto, los dem치s archivos existir치n solo localmente o en Google Cloud."

### 3. Completar las tablas de arquitectura

Las tablas de arquitectura determinan **cu치l es la estructura de
cada tabla de tu conjunto de datos**. Definen, por ejemplo, el nombre, orden y metadatos de las variables, adem치s de compatibilizaciones cuando hay cambios en versiones (por
ejemplo, si una variable cambia de nombre de un a침o a otro).

!!! Info "Cada tabla del conjunto de datos debe tener su propia tabla de arquitectura (hoja de c치lculo), que debe ser completada en **Google Drive** para permitir la correcci칩n por nuestro equipo de datos."

#### Ejemplo: RAIS - Tablas de arquitectura

Las tablas de arquitectura de RAIS [pueden ser consultadas aqu칤](https://docs.google.com/spreadsheets/d/1dPLUCeE4MSjs0ykYUDsFd-e7-9Nk6LVV/edit?usp=sharing&ouid=103008455637924805982&rtpof=true&sd=true). Son una excelente referencia para que empieces tu trabajo ya que tienen muchas variables y ejemplos de diversas situaciones que puedes encontrar.

#### Para completar cada tabla de tu conjunto sigue este paso a paso:

!!! Tip "Al inicio y final de cada etapa consulta nuestro [manual de estilo](style_data.md) para garantizar que est치s siguiendo la estandarizaci칩n de BD"

1. Listar todas las variables de los datos en la columna `original_name`
    - Obs: Si la base cambia el nombre de las variables a lo largo de los a침os (como RAIS), es necesario hacer la compatibilizaci칩n entre a침os para todas las variables completando la columna de `original_name_YYYY` para cada a침o o mes disponible
2. Renombrar las variables seg칰n nuestro [manual](style_data.md) en la columna `name`
3. Entender el tipo de variable y completar la columna `bigquery_type`
4. Completar la descripci칩n en `description` seg칰n el [manual](style_data.md)
5. A partir de la compatibilizaci칩n entre a침os y/o consultas a los datos brutos, completar la cobertura temporal en `temporal_coverage` de cada variable
    - Obs: Si las variables tienen la misma cobertura temporal que la tabla completar solo con '(1)'
6. Indicar con 'yes' o 'no' si hay diccionario para las variables en `covered_by_dictionary`
7. Verificar si las variables representan alguna entidad presente en los [directorios](https://basedosdados.org/dataset?q=diret%C3%B3rio&datasets_with=open_tables&organization=bd&page=1) para completar el `directory_column`
8. Para las variables del tipo `int64` o `float64` verificar si es necesario incluir una [unidad de medida](https://github.com/basedosdados/website/blob/master/ckanext-basedosdados/ckanext/basedosdados/validator/available_options/measurement_unit.py)
9. Reordenar las variables seg칰n el [manual](style_data.md)

!!! Tip "Cuando termines de completar las tablas de arquitectura, contacta con el equipo de Base de los Datos para validar todo. Es necesario que est칠 claro el formato final que los datos deben tener _antes_ de empezar a escribir el c칩digo. As칤 evitamos el retrabajo."

### 4. Escribir c칩digo de captura y limpieza de datos

Despu칠s de validadas las tablas de arquitectura, podemos escribir los c칩digos de
**captura** y **limpieza** de los datos.

- **Captura**: C칩digo que descarga autom치ticamente todos los datos originales y los guarda en `/input`. Estos datos pueden estar disponibles en portales o enlaces FTP, pueden ser raspados de sitios, entre otros.

- **Limpieza**: C칩digo que transforma los datos originales guardados en `/input` en datos limpios, guarda en la carpeta `/output`, para, posteriormente, ser subidos a BD.

Cada tabla limpia para producci칩n puede ser guardada como un archivo 칰nico o, si es muy grande (por ejemplo, por encima de 200 mb), ser particionada en el formato [Hive](https://cloud.google.com/bigquery/docs/hive-partitioned-loads-gcs) en varios sub-archivos. Los formatos aceptados son `.csv` o `.parquet`. Nuestra recomendaci칩n es particionar tablas por `ano`, `mes` y `sigla_uf`. El particionamiento se hace a trav칠s de la estructura de carpetas, ve el ejemplo abajo para visualizar c칩mo.

#### Ejemplo: RAIS - Particionamiento
La tabla `microdados_vinculos` de RAIS, por ejemplo, es una tabla muy grande (+250GB) por eso nosotros particionamos por `ano` y `sigla_uf`. El particionamiento se hizo usando la estructura de carpetas `/microdados_vinculos/ano=YYYY/sigla_uf=XX` .

#### Est치ndares necesarios en el c칩digo

- Deben ser escritos en [Python](https://www.python.org/),
  [R](https://www.r-project.org/) o [Stata](https://www.stata.com/) -
  para que la revisi칩n pueda ser realizada por el equipo.
- Puede estar en script (`.py`, `.R`, ...) o *notebooks* (Google Colab, Jupyter, Rmarkdown, etc).
- Las rutas de archivos deben ser atajos _relativos_ a la carpeta ra칤z
  (`<dataset_id>`), es decir, no deben depender de las rutas de tu
  computadora.
- La limpieza debe seguir nuestro [manual de estilo](style_data.md) y las [mejores pr치cticas de programaci칩n](https://en.wikipedia.org/wiki/Best_coding_practices).

#### Ejemplo: PNAD Continua - C칩digo de limpieza

El c칩digo de limpieza fue construido en R y [puede ser consultado
aqu칤](https://github.com/basedosdados/mais/tree/master/bases/br_ibge_pnadc/code).

#### Ejemplo: Actividad en la C치mara Legislativa - C칩digo de descarga y limpieza
El c칩digo de limpieza fue construido en Python [puede ser consultado aqu칤](https://github.com/basedosdados/mais/tree/bea9a323afcea8aa1609e9ade2502ca91f88054c/bases/br_camara_atividade_legislativa/code)

### 5. (Si es necesario) Organizar archivos auxiliares

Es com칰n que las bases de datos sean disponibilizadas con archivos auxiliares. Estos pueden incluir notas t칠cnicas, descripciones de recolecci칩n y muestreo, etc. Para ayudar a los usuarios de Base de los Datos a tener m치s contexto y entender mejor los datos, organiza todos estos archivos auxiliares en `/extra/auxiliary_files`.

Si칠ntete libre de estructurar sub-carpetas como quieras all칤 dentro. Lo que importa es que quede claro qu칠 son estos archivos.

### 6. (Si es necesario) Crear tabla diccionario

Muchas veces, especialmente con bases antiguas, hay m칰ltiples diccionarios en formatos Excel u otros. En Base de los Datos unificamos todo en un 칰nico archivo en formato `.csv` - un 칰nico diccionario para todas las columnas de todas las tablas de tu conjunto.

!!! Info "Detalles importantes de c칩mo construir tu diccionario est치n en nuestro [manual de estilo](../style_data/#dicionarios)."

#### Ejemplo: RAIS - Diccionario

El diccionario completo [puede ser consultado
aqu칤](https://docs.google.com/spreadsheets/d/12Wwp48ZJVux26rCotx43lzdWmVL54JinsNnLIV3jnyM/edit?usp=sharing).
Ya posee la estructura est치ndar que utilizamos para diccionarios.

### 7. Subir todo a Google Cloud

춰Todo listo! Ahora solo falta subir a Google Cloud y enviar para
revisi칩n. Para esto, vamos a usar el cliente `basedosdados` (disponible en Python) que facilita las configuraciones y etapas del proceso.

!!! Info "Como existe un costo para el almacenamiento en storage, para finalizar esta etapa necesitaremos proporcionarte una api_key espec칤fica para voluntarios para subir los datos en nuestro ambiente de desarrollo. As칤 que 칰nete a nuestro [canal en Discord](https://discord.gg/huKWpsVYx4) y ll치manos en 'quiero-contribuir'"

#### Configura tus credenciales localmente
**7.1** En tu terminal instala nuestro cliente: `pip install basedosdados`.

**7.2** Ejecuta `import basedosdados as bd` en python y sigue el paso a paso para configurar localmente con las credenciales de tu proyecto en Google Cloud. Completa la informaci칩n como sigue:
```
    * STEP 1: y
    * STEP 2: basedosdados-dev  (colocar el .json pasado por el equipo de bd en la carpeta credentials)
    * STEP 3: y
    * STEP 4: basedosdados-dev
    * STEP 5: https://api.basedosdados.org/api/v1/graphql
```
#### Sube los archivos a la Cloud
Los datos pasar치n por 3 lugares en Google Cloud:

  * **Storage**: tambi칠n llamado GCS es el lugar donde ser치n almacenados los archivos "fr칤os" (arquitecturas, datos, archivos auxiliares).
  * **BigQuery-DEV-Staging**: tabla que conecta los datos del storage al proyecto basedosdados-dev en bigquery
  * **BigQuery-DEV-Producci칩n**: tabla utilizada para pruebas y tratamiento v칤a SQL del conjunto de datos

**7.3** Crea la tabla en el *bucket del GCS* y *BigQuey-DEV-staging*, usando la API de Python, de la siguiente forma:

    ```python
    import basedosdados as bd

    tb = bd.Table(
      dataset_id='<dataset_id>',
      table_id='<table_id>')

    tb.create(
        path='<ruta_para_los_datos>',
        if_table_exists='raise',
        if_storage_data_exists='raise',
    )
    ```

    Los siguientes par치metros pueden ser usados:


    - `path` (obligatorio): la ruta completa del archivo en tu computadora, como: `/Users/<tu_usuario>/proyectos/basedosdados/mais/bases/[DATASET_ID]/output/microdados.csv`.


    !!! Tip "Si tus datos est치n particionados, la ruta debe apuntar a la carpeta donde est치n las particiones. En caso contrario, debe apuntar a un archivo `.csv` (por ejemplo, microdados.csv)."

    - `force_dataset`: comando que crea los archivos de configuraci칩n del dataset en BigQuery.
        - _True_: los archivos de configuraci칩n del dataset ser치n creados en tu proyecto y, si no existe en BigQuery, ser치 creado autom치ticamente. **Si ya has creado y configurado el dataset, no uses esta opci칩n, pues sobrescribir치 archivos**.
        - _False_: el dataset no ser치 recreado y, si no existe, ser치 creado autom치ticamente.
    - `if_table_exists` : comando utilizado si la **tabla ya existe en BQ**:
        - _raise_: retorna mensaje de error.
        - _replace_: sustituye la tabla.
        - _pass_: no hace nada.

    - `if_storage_data_exists`: comando utilizado si los **datos ya existen en Google Cloud Storage**:
        - _raise_: retorna mensaje de error
        - _replace_: sustituye los datos existentes.
        - _pass_: no hace nada.

    !!! Info "Si el proyecto no existe en BigQuery, ser치 autom치ticamente creado"

  Consulta tambi칠n nuestra [API](../api_reference_cli) para m치s detalles de cada m칠todo.

**7.4** Crea los archivos .sql y schema.yml a partir de la tabla de arquitectura siguiendo esta [documentaci칩n](https://github.com/basedosdados/pipelines/wiki/Fun%C3%A7%C3%A3o-%60create_yaml_file()%60)
!!! Tip "Si lo necesitas, en este momento puedes alterar la consulta en SQL para realizar tratamientos finales a partir de la tabla `staging`, puedes incluir columna, remover columna, hacer operaciones algebraicas, sustituir strings, etc. 춰El SQL es el l칤mite!"

**7.5** Ejecuta y prueba los modelos localmente siguiendo esta [documentaci칩n](https://github.com/basedosdados/pipelines/wiki/Testar-modelos-dbt-localmente)

**7.6** Sube los metadatos de la tabla en el sitio:
!!! Info "Por ahora solo el equipo de datos tiene permisos para subir los metadatos de la tabla en el sitio, por eso ser치 necesario contactar con nosotros. Ya estamos trabajando para que, en un futuro pr칩ximo, los voluntarios tambi칠n puedan actualizar datos en el sitio."

**7.7** Sube los archivos auxiliares:
    ```python
    st = bd.Storage(
      dataset_id = <dataset_id>,
      table_id = <table_id>)

    st.upload(
      path='ruta_para_los_archivos_auxiliares',
      mode = 'auxiliary_files',
      if_exists = 'raise')
    ```

### 8. Enviar todo para revisi칩n

춰Uf, eso es todo! Ahora solo queda enviar todo para revisi칩n en el
[repositorio](https://github.com/basedosdados/queries-basedosdados) de Base de los Datos.

1. Clona nuestro [repositorio](https://github.com/basedosdados/queries-basedosdados) localmente.
2. Da un `cd` a la carpeta local del repositorio y abre una nueva branch con `git checkout -b [dataset_id]`. Todas las adiciones y modificaciones ser치n incluidas en esa _branch_.
3. Para cada tabla nueva incluir el archivo con nombre `table_id.sql` en la carpeta `queries-basedosdados/models/dataset_id/` copiando las queries que desarrollaste en el paso 7.
4. Incluir el archivo schema.yaml desarrollado en el paso 7
5. Si es un dataset nuevo, incluir el dataset conforme las instrucciones del archivo `queries-basedosdados/dbt_project.yaml` (no te olvides de seguir el orden alfab칠tico para no desordenar la organizaci칩n)
6. Incluye tu c칩digo de captura y limpieza en la carpeta `queries-basedosdados/models/dataset_id/code`
7. Ahora solo falta publicar la branch, abrir el PR con las labels 'table-approve' y marcar al equipo de datos para correcci칩n

**쯏 ahora?** Nuestro equipo revisar치 los datos y metadatos enviados
v칤a Github. Podemos contactarte para resolver dudas o solicitar
cambios en el c칩digo. Cuando todo est칠 OK, hacemos un _merge_ de tu
*pull request* y los datos son autom치ticamente publicados en nuestra
plataforma!
