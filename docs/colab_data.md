

# Suba dados na BD

## Por que minha organiza√ß√£o deve subir dados na BD?

- **Capacidade de cruzar suas bases com dados de diferentes
  organiza√ß√µes** de forma simples e f√°cil. J√° s√£o centenas de conjuntos
  de dados p√∫blicos das maiores organiza√ß√µes do Brasil e do mundo presentes
  no nosso *datalake*.

- **Compromisso com a transpar√™ncia, qualidade dos dados e
  desenvolvimento de melhores pesquisas, an√°lises e solu√ß√µes** para a
  sociedade. N√£o s√≥ democratizamos o acesso a dados abertos, mas tamb√©m dados
  de qualidade. Temos um time especializado que revisa e garante a qualidade dos
  dados adicionados ao *datalake*.

- **Participa√ß√£o de uma comunidade que cresce cada vez mais**: milhares
  de jornalistas, pesquisadores(as), desenvolvedores(as), j√° utilizam e
  acompanham a Base dos Dados.
  <!-- TODO: Colocar aqui o link do nosso painel de audiencia quando tiver pronto :) -->

## Passo a passo para subir dados

Quer subir dados na BD e nos ajudar a construir esse reposit√≥rio?
*Maravilha!* Organizamos tudo o que voc√™ precisa no manual abaixo em 7 passos

Para facilitar a explica√ß√£o, vamos seguir um exemplo j√° pronto com dados da [RAIS](https://basedosdados.org/dataset/br-me-rais).

!!! Tip "Voc√™ pode navegar pelas etapas no menu √† esquerda."
    Sugerimos fortemente que entre em nosso [canal no
    Discord](https://discord.gg/huKWpsVYx4) para tirar d√∫vidas e
    interagir com a equipe e outros(as) colaboradores(as)! üòâ

### Antes de come√ßar

Alguns conhecimentos s√£o necess√°rias para realizar esse processo:

- **Python, R, SQL e/ou Stata**: para criar os c√≥digos de captura e limpeza dos dados.
- **Linha de comando**: para configurar seu ambiente local
  e conex√£o com o Google Cloud.
- **Github**: para subir seu c√≥digo para revis√£o da
  nossa equipe.

!!! Tip "N√£o tem alguma dessas habilidades, mas quer colaborar?"
    Temos um time de dados que pode te ajudar, basta entrar no [nosso
    Discord](https://discord.gg/huKWpsVYx4) e mandar uma mensagem em #quero-contribuir.

### Como funciona o processo?

[1. Escolher a base e entender mais dos dados](#1-Escolher-a-base-e-entender-mais-dos-dados) - primeiro precisamos conhecer o que estamos tratando.  
[2. Baixar nossa pasta template](#2-Baixar-nossa-pasta-template) - √© hora estruturar o trabalho a ser feito    
[3. Preencher as tabelas de arquitetura](#3-preencher-as-tabelas-de-arquitetura) - √© primordial definir a estrutura dos dados antes de iniciarmos o tratamento   
[4. Escrever codigo de captura e limpeza de dados](#4-escrever-codigo-de-captura-e-limpeza-de-dados) - hora de botar a m√£o na massa (ou no teclado)!  
[5. (Caso necess√°rio) Organizar arquivos auxiliares](#5-caso-necess√°rio-organizar-arquivos-auxiliares) -  porque at√© dados precisam de guias  
[6. (Caso necess√°rio) Criar tabela dicion√°rio](#6-caso-necess√°rio-criar-tabela-dicion√°rio) - padronizamos os dicion√°rios pra vida do usu√°rio ficar mais f√°cil  
[7. Subir tudo no Google Cloud](#7-subir-tudo-no-google-cloud) - afinal, √© por l√° que ficam os dados da BD  
[8. Enviar tudo para revis√£o](#8-enviar-tudo-para-revis√£o) - um olhar da nossa equipe para garantir que tudo est√° perfeito!


### 1. Escolher a base e entender mais dos dados

Mantemos a lista de conjuntos para volunt√°rios no nosso [Github](https://github.com/orgs/basedosdados/projects/17/views/10). Para come√ßar a subir uma base do seu interesse, basta abrir uma [nova issue](https://github.com/basedosdados/queries-basedosdados/issues/new?assignees=&labels=&projects=&template=issue--novos-dados.md&title=%5Bdados%5D+%3Cdataset_id%3E) de dados. Caso sua base (conjunto) j√° esteja listada, basta marcar seu usu√°rio do Github como `assignee`

Seu primeiro trabalho √© preencher as informa√ß√µes na issue. Essas informa√ß√µes v√£o te ajudar a entender melhor os dados e ser√£o muito √∫teis para o tratamento e o preenchimento de metadados.

Quando finalizar essa etapa, chame algu√©m da equipe dados para que as informa√ß√µes que voc√™ mapeou sobre o conjunto j√° entrem pro nosso site!
### 2. Baixar nossa pasta template

[Baixe aqui a pasta
_template_](https://drive.google.com/drive/folders/1xXXon0vdjSKr8RCNcymRdOKgq64iqfS5?usp=sharing)
 e renomeie para o `<dataset_id>` (definido na issue do [passo 1](#-1-Escolher-a-base-e-entender-mais-dos-dados)). Essa pasta template facilita e organiza todos os
passos daqui pra frente. Sua
estrutura √© a seguinte:

- `<dataset_id>/`
    - `code/`: C√≥digos necess√°rios para **captura** e **limpeza** dos dados
    ([vamos ver mais no passo
    4](#4-escrever-codigo-de-captura-e-limpeza-de-dados)).
    - `input/`: Cont√©m todos os arquivos com dados originais, exatamente
    como baixados da fonte prim√°ria. ([vamos ver mais no passo
    4](#4-escrever-codigo-de-captura-e-limpeza-de-dados)).
    - `output/`: Arquivos finais, j√° no formato pronto para subir na BD ([vamos ver mais no passo
    4](#4-escrever-codigo-de-captura-e-limpeza-de-dados)).
    - `tmp/`: Quaisquer arquivos tempor√°rios criados pelo c√≥digo em `/code` no processo de limpeza e tratamento ([vamos ver mais no passo
    4](#4-escrever-codigo-de-captura-e-limpeza-de-dados)).
    - `extra/`
        - `architecture/`: Tabelas de arquitetura ([vamos ver mais no passo 3](#3-preencher-as-tabelas-de-arquitetura)).
        - `auxiliary_files/`: Arquivos auxiliares aos dados ([vamos ver mais no passo 5](#5-caso-necess√°rio-organizar-arquivos-auxiliares)).
        - `dicionario.csv`: Tabela dicion√°rio de todo o conjunto de dados ([vamos ver mais no passo 6](#6-caso-necess√°rio-criar-tabela-dicion√°rio)).

!!! info "Apenas a pasta `code` ser√° commitada para o seu projeto, os demais arquivos existir√£o apenas localmente ou no Google Cloud."

### 3. Preencher as tabelas de arquitetura

As tabelas de arquitetura determinam **qual a estrutura de
cada tabela do seu conjunto de dados**. Elas definem, por exemplo, o nome, ordem e metadados das vari√°veis, al√©m de compatibiliza√ß√µes quando h√° mudan√ßas em vers√µes (por
exemplo, se uma vari√°vel muda de nome de um ano para o outro).

!!! Info "Cada tabela do conjunto de dados deve ter sua pr√≥pria tabela de arquitetura (planilha), que deve ser preenchida no **Google Drive** para permitir a corre√ß√£o pela nossa equipe de dados."


Para cada tabela do seu conjunto siga esse passo a passo:

1. Listar todas as vari√°veis dos dados na coluna `original_name`
2. Caso a base tenha arquivos diferentes e mude o nome das vari√°veis ao longo dos anos, fazer a compatibiliza√ß√£o entre anos para todas as vari√°veis preenchendo a coluna de `original_name_YYYY` para cada ano ou m√™s dispon√≠vel
3. Renomear as vari√°veis conforme nosso manual na coluna `name`  
4. Entender o tipo da vari√°vel e preencher a coluna `bigquery_type`
5. Preencher a descri√ß√£o em `description` conforme o manual
6. A partir da compatibiliza√ß√£o entre anos e/ou consultas aos dados brutos, preencher a cobertura temporal em `temporal_coverage` de cada vari√°vel 
7. Indicar se h√° dicion√°rio para as vari√°veis categ√≥ricas em `covered_by_dictionary`
8. Verificar se as vari√°veis representam alguma entidade presente nos [diret√≥rios](https://basedosdados.org/dataset?q=diret%C3%B3rio&datasets_with=open_tables&organization=bd&page=1) para preencher o `directory_column`
9. Para as vari√°veis do tipo `int64` ou `float64` verificar se √© necess√°rio incluir uma [unidade de medida](https://github.com/basedosdados/website/blob/master/ckanext-basedosdados/ckanext/basedosdados/validator/available_options/measurement_unit.py)
10. Reordernar as vari√°veis conforme o manual

!!! Tip "A cada in√≠cio e final de etapa consulte nosso [manual de estilo](../style_data) para garantir que voc√™ est√° seguindo a padroniza√ß√£o da BD"

#### Exemplo: RAIS - Tabelas de arquitetura

As tabelas de arquitetura da RAIS [podem ser consultadas aqui](https://docs.google.com/spreadsheets/d/1dPLUCeE4MSjs0ykYUDsFd-e7-9Nk6LVV/edit?usp=sharing&ouid=103008455637924805982&rtpof=true&sd=true). Elas s√£o uma √≥tima refer√™ncia para voc√™ come√ßar seu trabalho j√° que tem muitas vari√°veis e exemplos de diversas situa√ß√µes que voc√™ pode acabar encontrando.


!!! Tip "Quando terminar de preencher as tabelas de arquitetura, entre em contato com a equipe da Base dos Dados para validar tudo. √â necess√°rio que esteja claro o formato final que os dados devem ficar _antes_ de come√ßar a escrever o c√≥digo. Assim a gente evita o retrabalho."

### 4. Escrever codigo de captura e limpeza de dados

Ap√≥s validadas as tabelas de arquitetura, podemos escrever os c√≥digos de
**captura** e **limpeza** dos dados.

- **Captura**: C√≥digo que baixa automaticamente todos os dados originais e os salva em `/input`. Esses dados podem estar dispon√≠veis em portais ou links FTP, podem ser raspados de sites, entre outros.

- **Limpeza**: C√≥digo que transforma os dados originais salvos em `/input` em dados limpos, salva na pasta `/output`, para, posteriormente, serem subidos na BD.

Cada tabela limpa para produ√ß√£o pode ser salva como um arquivo √∫nico ou, caso seja muito grande (e.g. acima de 200 mb), ser particionada no formato [Hive](https://cloud.google.com/bigquery/docs/hive-partitioned-loads-gcs) em v√°rios sub-arquivos. Os formatos aceitos s√£o `.csv` ou `.parquet`. Nossa recomenda√ß√£o √© particionar tabelas por `ano`, `mes` e `sigla_uf`. O particionamento √© feito atrav√©s da estrutura de pastas, veja o exemplo a baixo para visualizar como.

#### Exemplo: RAIS - Particionamento
A tabela `microdados_vinculos` da RAIS, por exemplo, √© uma tabela muito grande (+250GB) por isso n√≥s particionamos por `ano` e `sigla_uf`. O particionamento foi feito usando a estrutura de pastas `/microdados_vinculos/ano=YYYY/sigla_uf=XX` .

#### Padr√µes necess√°rios no c√≥digo

- Devem ser escritos em [Python](https://www.python.org/),
  [R](https://www.r-project.org/) ou [Stata](https://www.stata.com/) -
  para que a revis√£o possa ser realizada pela equipe.
- Pode estar em script (`.py`, `.R`, ...) ou *notebooks* (Google Colab, Jupyter, Rmarkdown, etc).
- Os caminhos de arquivos devem ser atalhos _relativos_ √† pasta ra√≠z
  (`<dataset_id>`), ou seja, n√£o devem depender dos caminhos do seu
  computador.
- A limpeza deve seguir nosso [manual de estilo](../style_data) e as [melhores pr√°ticas de programa√ß√£o](https://en.wikipedia.org/wiki/Best_coding_practices).

#### Exemplo: PNAD Cont√≠nua - C√≥digo de limpeza

O c√≥digo de limpeza foi constru√≠do em R e [pode ser consultado
aqui](https://github.com/basedosdados/mais/tree/master/bases/br_ibge_pnadc/code).

#### Exemplo: Atividade na C√¢mara Legislativa - C√≥digo de download e limpeza
O c√≥digo de limpeza foi constru√≠do em Python [pode ser consultado aqui](https://github.com/basedosdados/mais/tree/bea9a323afcea8aa1609e9ade2502ca91f88054c/bases/br_camara_atividade_legislativa/code)



### 5. (Caso necess√°rio) Organizar arquivos auxiliares

√â comum bases de dados serem disponibilizadas com arquivos auxiliares. Esses podem incluir notas t√©cnicas, descri√ß√µes de coleta e amostragem, etc. Para ajudar usu√°rios da Base dos Dados terem mais contexto e entenderem melhor os dados, organize todos esses arquivos auxiliares em `/extra/auxiliary_files`.

Fique √† vontade para estruturar sub-pastas como quiser l√° dentro. O que importa √© que fique claro o que s√£o esses arquivos.

### 6. (Caso necess√°rio) Criar tabela dicion√°rio

Muitas vezes, especialmente com bases antigas, h√° m√∫ltiplos dicion√°rios em formatos Excel ou outros. Na Base dos Dados n√≥s unificamos tudo em um √∫nico arquivo em formato `.csv` - um √∫nico dicion√°rio para todas as colunas de todas as tabelas do seu conjunto.

!!! Info "Detalhes importantes de como construir seu dicion√°rio est√£o no nosso [manual de estilo](../style_data/#dicionarios)."

#### Exemplo: RAIS - Dicion√°rio

O dicion√°rio completo [pode ser consultado
aqui](https://docs.google.com/spreadsheets/d/12Wwp48ZJVux26rCotx43lzdWmVL54JinsNnLIV3jnyM/edit?usp=sharing).
Ele j√° possui a estrutura padr√£o que utilizamos para dicion√°rios.

### 7. Subir tudo no Google Cloud

Tudo pronto! Agora s√≥ falta subir para o Google Cloud e enviar para
revis√£o. Para isso, vamos usar o cliente `basedosdados` (dispon√≠vel em Python) que facilita as configura√ß√µes e etapas do processo.

#### Configure suas credenciais localmente
1. No seu terminal instale nosso cliente: `pip install basedosdados`.
2. Rode `import basedosdados as bd` no python e siga o passo a passo para configurar localmente com as credenciais de seu projeto no Google Cloud.

#### Configure seu projeto no Google Cloud e um _bucket_ no Google Storage

Os dados v√£o passar ao todo por 3 lugares no Google Cloud:

1. **Storage**: local onde ser√£o armazenados o arquivos "frios" (arquiteturas, dados, arquivos auxiliares).
2. **BigQuery**: super banco de dados do Google, dividido em 2 projetos/tipos de tabela:
    - *Staging*: banco para teste e tratamento final do conjunto de dados.
    - *Produ√ß√£o*: banco oficial de publica√ß√£o dos dados (nosso projeto `basedosdados` ou o seu mesmo caso queira reproduzir o ambiente)

√â necess√°rio ter uma conta Google e um projeto (gratuito) no Google
Cloud. Para criar seu projeto basta:

1. Acessar o [link](https://console.cloud.google.com/projectselector2/home/dashboard) e aceitar o Termo de Servi√ßos do Google Cloud.
2. Clicar em `Create Project/Criar Projeto` - escolha um nome bacana para o seu projeto, ele ter√° tamb√©m um `Project ID` que ser√° utilizado para configura√ß√£o local.
3. Depois de criado o projeto, v√° at√© a funcionalidade de
   [Storage](https://console.cloud.google.com/storage) e crie uma pasta
   (tamb√©m chamado de _bucket_) para subir os dados (pode ter o mesmo nome do projeto).
4. Garanta que seu projeto √© p√∫blico, isso √© necess√°rio para que a gente consiga copiar os dados que estar√£o no seu projeto para o nosso ambiente de Cloud


#### Suba os arquivos na Cloud


1. Crie a tabela no *bucket do GCS* e *BigQuey*, usando a API do Python, da seguinte forma:

    ```python
    import basedosdados as bd
    
    tb = bd.Table(
      dataset_id='<dataset_id>', 
      table_id='<table_id>')

    tb.create(
        path='<caminho_para_os_dados>',
        if_table_exists='raise',
        if_storage_data_exists='raise',
    )
    ```

    Os seguintes par√¢metros podem ser usados:

    
    - `path` (obrigat√≥rio): o caminho completo do arquivo no seu computador, como: `/Users/<seu_usuario>/projetos/basedosdados/mais/bases/[DATASET_ID]/output/microdados.csv`. 
    
    
    !!! Tip "Caso seus dados sejam particionados, o caminho deve apontar para a pasta onde est√£o as parti√ß√µes. No contr√°rio, deve apontar para um arquivo `.csv` (por exemplo, microdados.csv)."
  
    - `force_dataset`: comando que cria os arquivos de configura√ß√£o do dataset no BigQuery.
        - _True_: os arquivos de configura√ß√£o do dataset ser√£o criados no seu projeto e, caso ele n√£o exista no BigQuery, ser√° criado automaticamente. **Se voc√™ j√° tiver criado e configurado o dataset, n√£o use esta op√ß√£o, pois ir√° sobrescrever arquivos**.
        - _False_: o dataset n√£o ser√° recriado e, se n√£o existir, ser√° criado automaticamente.   
    - `if_table_exists` : comando utilizado caso a **tabela j√° exista no BQ**:    
        - _raise_: retorna mensagem de erro.
        - _replace_: substitui a tabela. 
        - _pass_: n√£o faz nada.

    - `if_storage_data_exists`: comando utilizado caso os **dados j√° existam no Google Cloud Storage**:
        - _raise_: retorna mensagem de erro
        - _replace_: substitui os dados existentes.
        - _pass_: n√£o faz nada.

    !!! Info "Se o projeto n√£o existir no BigQuery, ele ser√° automaticamente criado"

2. Suba os metadadaos da tabela no site:

    - Os dados do conjunto j√° foram disponibilizados no  primeiro passo desse tutorial, agora √© o momento de incluir os dados das tabelas.
    - Primeiro verifique que todas as informa√ß√µes da issue e da arquitetura est√£o de acordo com seus dados no BigQuery, as vezes, no tratamento, voc√™ modificou algumas coisas e esqueceu de atualizar nesses arquivos. 
    - Em seguida, pe√ßa para algu√©m da equipe dados subir os metadados da tabela no site com status `under_review`. J√° estamos trabalhando para que, num futuro pr√≥ximo, os volunt√°rios tamb√©m possam atualizar dados no site.  

3. Fa√ßa a query de publica√ß√£o para ver como a tabela ficar√° em prod :

    ```python
    tb = bd.Table(
      dataset_id="<dataset_id>", 
      table_id="<table_id>")

    tb.publish()
    ```
   Se os metadados de colunas j√° tiverem sido preenchidos no site e o pacote j√° vai montar um arquivo `.sql` certinho pra gente levar os dados de staging para produ√ß√£o.
   
   Caso voc√™ precise, nesse momento voc√™ pode alterar a consulta em SQL para realizar tratamentos finais na tabela `staging`, pode incluir coluna, remover coluna, fazer opera√ß√µes alg√©bricas, substituir strings, etc. O SQL √© o limite!

4. Suba os arquivos auxiliares:
    ```python
    st = bd.Storage(
      dataset_id = <dataset_id>, 
      table_id = <table_id>)

    st.upload(
      path='caminho_para_os_arquivos_auxiliares',
      mode = 'auxiliary_files',
      if_exists = 'raise')
    ```

!!! Tip "Abra o console do BigQuery e rode algumas _queries_ para testar se foi tudo publicado corretamente. Estamos desenvolvendo testes autom√°ticos para facilitar esse processo no futuro."

Quando os dados estiverem certos e organizados conforme a tabela de arquitetura vamos para a pr√≥xima etapa

Consulte tamb√©m nossa [API](../api_reference_cli) para mais detalhes de cada m√©todo.

### 8. Enviar tudo para revis√£o

Ufa, √© isso! Agora s√≥ resta enviar tudo para revis√£o no
[reposit√≥rio](https://github.com/basedosdados/queries-basedosdados) da Base dos Dados.

1. Clone um _fork_ do nosso [reposit√≥rio](https://github.com/basedosdados/queries-basedosdados) localmente.
2. D√™ um `cd` para a pasta local do reposit√≥rio e abra uma nova branch com `git checkout -b [dataset_id]`. Todas as adi√ß√µes e modifica√ß√µes
  ser√£o inclu√≠das nessa _branch_.
1. Para cada tabela nova incluir o arquivo com nome `table_id.sql` na pasta `queries-basedosdados/models/dataset_id/` copiando as queries que voc√™ desenvolveu no passo 7.
2. Caso seja um dataset novo, incluir o dataset conforme as instru√ß√µes do arquivo `queries-basedosdados/dbt_project.yaml` (n√£o se esque√ßa de seguir a ordem alfab√©tica pra n√£o bagun√ßar a organiza√ß√£o)
3. Inclua o seu c√≥digo de tratamento em na pasta `queries-basedosdados/models/dataset_id/code`
4. Agora √© s√≥ publicar a branch, abrir o PR com as labels 'table-approve' e 'sync-dbt-schema'e marcar a equipe dados para corre√ß√£o

**E agora?** Nossa equipe ir√° revisar os dados e metadados submetidos
via Github. Podemos entrar em contato para tirar d√∫vidas ou solicitar
mudan√ßas no c√≥digo. Quando tudo estiver OK, fazemos um _merge_ do seu
*pull request* e os dados s√£o automaticamente publicados na nossa
plataforma!
