

# Suba dados na BD

## Por que minha organiza√ß√£o deve subir dados na BD?

- **Capacidade de cruzar suas bases com dados de diferentes
  organiza√ß√µes** de forma simples e f√°cil. J√° s√£o centenas de conjuntos
  de dados p√∫blicos das maiores organiza√ß√µes do Brasil e do mundo presentes
  no nosso *datalake*.

- **Compromisso com a transpar√™ncia, qualidade dos dados e
  desenvolvimento de melhores pesquisas, an√°lises e solu√ß√µes** para a
  sociedade. N√£o s√≥ democratizamos o acesso a dados abertos, mas tamb√©m dados
  de qualidade. Temos um time especializado que revisa e garante a qualidade dos
  dados adicionados ao *datalake*.

- **Participa√ß√£o de uma comunidade que cresce cada vez mais**: milhares
  de jornalistas, pesquisadores(as), desenvolvedores(as), j√° utilizam e
  acompanham a Base dos Dados.
  <!-- TODO: Colocar aqui o link do nosso painel de audiencia quando tiver pronto :) -->

## Passo a passo para subir dados

Quer subir dados na BD e nos ajudar a construir esse reposit√≥rio?
*Maravilha!* Organizamos tudo o que voc√™ precisa no manual abaixo em 8 passos

Para facilitar a explica√ß√£o, vamos seguir um exemplo j√° pronto com dados da [RAIS](https://basedosdados.org/dataset/br-me-rais).

!!! Tip "Voc√™ pode navegar pelas etapas no menu √† esquerda."
    Sugerimos fortemente que entre em nosso [canal no
    Discord](https://discord.gg/huKWpsVYx4) para tirar d√∫vidas e
    interagir com a equipe e outros(as) colaboradores(as)! üòâ

### Antes de come√ßar

Alguns conhecimentos s√£o necess√°rias para realizar esse processo:

- **Python, R, SQL e/ou Stata**: para criar os c√≥digos de captura e limpeza dos dados.
- **Linha de comando**: para configurar seu ambiente local
  e conex√£o com o Google Cloud.
- **Github**: para subir seu c√≥digo para revis√£o da
  nossa equipe.

!!! Tip "N√£o tem alguma dessas habilidades, mas quer colaborar?"
    Temos um time de dados que pode te ajudar, basta entrar no [nosso
    Discord](https://discord.gg/huKWpsVYx4) e mandar uma mensagem em #quero-contribuir.

### Como funciona o processo?

[1. Escolher a base e entender mais dos dados](#1-Escolher-a-base-e-entender-mais-dos-dados) - primeiro precisamos conhecer o que estamos tratando.  
[2. Baixar nossa pasta template](#2-Baixar-nossa-pasta-template) - √© hora estruturar o trabalho a ser feito    
[3. Preencher as tabelas de arquitetura](#3-preencher-as-tabelas-de-arquitetura) - √© primordial definir a estrutura dos dados antes de iniciarmos o tratamento   
[4. Escrever codigo de captura e limpeza de dados](#4-escrever-codigo-de-captura-e-limpeza-de-dados) - hora de botar a m√£o na massa!  
[5. (Caso necess√°rio) Organizar arquivos auxiliares](#5-caso-necess√°rio-organizar-arquivos-auxiliares) -  porque at√© dados precisam de guias  
[6. (Caso necess√°rio) Criar tabela dicion√°rio](#6-caso-necess√°rio-criar-tabela-dicion√°rio) - momento de montar os dicion√°rios  
[7. Subir tudo no Google Cloud](#7-subir-tudo-no-google-cloud) - afinal, √© por l√° que ficam os dados da BD  
[8. Enviar tudo para revis√£o](#8-enviar-tudo-para-revis√£o) - um olhar da nossa equipe para garantir que tudo est√° pronto para ir pra produ√ß√£o!


### 1. Escolher a base e entender mais dos dados

Mantemos a lista de conjuntos para volunt√°rios no nosso [Github](https://github.com/orgs/basedosdados/projects/17/views/10). Para come√ßar a subir uma base do seu interesse, basta abrir uma [nova issue](https://github.com/basedosdados/queries-basedosdados/issues/new?assignees=&labels=&projects=&template=issue--novos-dados.md&title=%5Bdados%5D+%3Cdataset_id%3E) de dados. Caso sua base (conjunto) j√° esteja listada, basta marcar seu usu√°rio do Github como `assignee`

Seu primeiro trabalho √© preencher as informa√ß√µes na issue. Essas informa√ß√µes v√£o te ajudar a entender melhor os dados e ser√£o muito √∫teis para o tratamento e o preenchimento de metadados.

Quando finalizar essa etapa, chame algu√©m da equipe dados para que as informa√ß√µes que voc√™ mapeou sobre o conjunto j√° entrem pro nosso site!
### 2. Baixar nossa pasta template

[Baixe aqui a pasta
_template_](https://drive.google.com/drive/folders/1xXXon0vdjSKr8RCNcymRdOKgq64iqfS5?usp=sharing)
 e renomeie para o `<dataset_id>` (definido na issue do [passo 1](#-1-Escolher-a-base-e-entender-mais-dos-dados)). Essa pasta template facilita e organiza todos os
passos daqui pra frente. Sua
estrutura √© a seguinte:

- `<dataset_id>/`
    - `code/`: C√≥digos necess√°rios para **captura** e **limpeza** dos dados
    ([vamos ver mais no passo
    4](#4-escrever-codigo-de-captura-e-limpeza-de-dados)).
    - `input/`: Cont√©m todos os arquivos com dados originais, exatamente
    como baixados da fonte prim√°ria. ([vamos ver mais no passo
    4](#4-escrever-codigo-de-captura-e-limpeza-de-dados)).
    - `output/`: Arquivos finais, j√° no formato pronto para subir na BD ([vamos ver mais no passo
    4](#4-escrever-codigo-de-captura-e-limpeza-de-dados)).
    - `tmp/`: Quaisquer arquivos tempor√°rios criados pelo c√≥digo em `/code` no processo de limpeza e tratamento ([vamos ver mais no passo
    4](#4-escrever-codigo-de-captura-e-limpeza-de-dados)).
    - `extra/`
        - `architecture/`: Tabelas de arquitetura ([vamos ver mais no passo 3](#3-preencher-as-tabelas-de-arquitetura)).
        - `auxiliary_files/`: Arquivos auxiliares aos dados ([vamos ver mais no passo 5](#5-caso-necess√°rio-organizar-arquivos-auxiliares)).
        - `dicionario.csv`: Tabela dicion√°rio de todo o conjunto de dados ([vamos ver mais no passo 6](#6-caso-necess√°rio-criar-tabela-dicion√°rio)).

!!! info "Apenas a pasta `code` ser√° commitada para o seu projeto, os demais arquivos existir√£o apenas localmente ou no Google Cloud."

### 3. Preencher as tabelas de arquitetura

As tabelas de arquitetura determinam **qual a estrutura de
cada tabela do seu conjunto de dados**. Elas definem, por exemplo, o nome, ordem e metadados das vari√°veis, al√©m de compatibiliza√ß√µes quando h√° mudan√ßas em vers√µes (por
exemplo, se uma vari√°vel muda de nome de um ano para o outro).

!!! Info "Cada tabela do conjunto de dados deve ter sua pr√≥pria tabela de arquitetura (planilha), que deve ser preenchida no **Google Drive** para permitir a corre√ß√£o pela nossa equipe de dados."


#### Exemplo: RAIS - Tabelas de arquitetura

As tabelas de arquitetura da RAIS [podem ser consultadas aqui](https://docs.google.com/spreadsheets/d/1dPLUCeE4MSjs0ykYUDsFd-e7-9Nk6LVV/edit?usp=sharing&ouid=103008455637924805982&rtpof=true&sd=true). Elas s√£o uma √≥tima refer√™ncia para voc√™ come√ßar seu trabalho j√° que tem muitas vari√°veis e exemplos de diversas situa√ß√µes que voc√™ pode acabar encontrando.

#### Para o preenchimento de cada tabela do seu conjunto siga esse passo a passo:

!!! Tip "A cada in√≠cio e final de etapa consulte nosso [manual de estilo](../style_data) para garantir que voc√™ est√° seguindo a padroniza√ß√£o da BD"

1. Listar todas as vari√°veis dos dados na coluna `original_name`
   - Obs: Caso a base mude o nome das vari√°veis ao longo dos anos (como a RAIS), √© necess√°rio fazer a compatibiliza√ß√£o entre anos para todas as vari√°veis preenchendo a coluna de `original_name_YYYY` para cada ano ou m√™s dispon√≠vel
2. Renomear as vari√°veis conforme nosso [manual](../style_data) na coluna `name`
3. Entender o tipo da vari√°vel e preencher a coluna `bigquery_type`
4. Preencher a descri√ß√£o em `description` conforme o [manual](../style_data)
5. A partir da compatibiliza√ß√£o entre anos e/ou consultas aos dados brutos, preencher a cobertura temporal em `temporal_coverage` de cada vari√°vel
   - Obs: Caso a as vari√°veis tenham a mesma cobertura temporal da tabela preencher apenas com '(1)'
6. Indicar com 'yes' ou 'no' se h√° dicion√°rio para as vari√°veis em `covered_by_dictionary`
7. Verificar se as vari√°veis representam alguma entidade presente nos [diret√≥rios](https://basedosdados.org/dataset?q=diret%C3%B3rio&datasets_with=open_tables&organization=bd&page=1) para preencher o `directory_column`
8 Para as vari√°veis do tipo `int64` ou `float64` verificar se √© necess√°rio incluir uma [unidade de medida](https://github.com/basedosdados/website/blob/master/ckanext-basedosdados/ckanext/basedosdados/validator/available_options/measurement_unit.py)
9. Reordernar as vari√°veis conforme o [manual](../style_data)

!!! Tip "Quando terminar de preencher as tabelas de arquitetura, entre em contato com a equipe da Base dos Dados para validar tudo. √â necess√°rio que esteja claro o formato final que os dados devem ficar _antes_ de come√ßar a escrever o c√≥digo. Assim a gente evita o retrabalho."

### 4. Escrever codigo de captura e limpeza de dados

Ap√≥s validadas as tabelas de arquitetura, podemos escrever os c√≥digos de
**captura** e **limpeza** dos dados.

- **Captura**: C√≥digo que baixa automaticamente todos os dados originais e os salva em `/input`. Esses dados podem estar dispon√≠veis em portais ou links FTP, podem ser raspados de sites, entre outros.

- **Limpeza**: C√≥digo que transforma os dados originais salvos em `/input` em dados limpos, salva na pasta `/output`, para, posteriormente, serem subidos na BD.

Cada tabela limpa para produ√ß√£o pode ser salva como um arquivo √∫nico ou, caso seja muito grande (e.g. acima de 200 mb), ser particionada no formato [Hive](https://cloud.google.com/bigquery/docs/hive-partitioned-loads-gcs) em v√°rios sub-arquivos. Os formatos aceitos s√£o `.csv` ou `.parquet`. Nossa recomenda√ß√£o √© particionar tabelas por `ano`, `mes` e `sigla_uf`. O particionamento √© feito atrav√©s da estrutura de pastas, veja o exemplo a baixo para visualizar como.

#### Exemplo: RAIS - Particionamento
A tabela `microdados_vinculos` da RAIS, por exemplo, √© uma tabela muito grande (+250GB) por isso n√≥s particionamos por `ano` e `sigla_uf`. O particionamento foi feito usando a estrutura de pastas `/microdados_vinculos/ano=YYYY/sigla_uf=XX` .

#### Padr√µes necess√°rios no c√≥digo

- Devem ser escritos em [Python](https://www.python.org/),
  [R](https://www.r-project.org/) ou [Stata](https://www.stata.com/) -
  para que a revis√£o possa ser realizada pela equipe.
- Pode estar em script (`.py`, `.R`, ...) ou *notebooks* (Google Colab, Jupyter, Rmarkdown, etc).
- Os caminhos de arquivos devem ser atalhos _relativos_ √† pasta ra√≠z
  (`<dataset_id>`), ou seja, n√£o devem depender dos caminhos do seu
  computador.
- A limpeza deve seguir nosso [manual de estilo](../style_data) e as [melhores pr√°ticas de programa√ß√£o](https://en.wikipedia.org/wiki/Best_coding_practices).

#### Exemplo: PNAD Cont√≠nua - C√≥digo de limpeza

O c√≥digo de limpeza foi constru√≠do em R e [pode ser consultado
aqui](https://github.com/basedosdados/mais/tree/master/bases/br_ibge_pnadc/code).

#### Exemplo: Atividade na C√¢mara Legislativa - C√≥digo de download e limpeza
O c√≥digo de limpeza foi constru√≠do em Python [pode ser consultado aqui](https://github.com/basedosdados/mais/tree/bea9a323afcea8aa1609e9ade2502ca91f88054c/bases/br_camara_atividade_legislativa/code)



### 5. (Caso necess√°rio) Organizar arquivos auxiliares

√â comum bases de dados serem disponibilizadas com arquivos auxiliares. Esses podem incluir notas t√©cnicas, descri√ß√µes de coleta e amostragem, etc. Para ajudar usu√°rios da Base dos Dados terem mais contexto e entenderem melhor os dados, organize todos esses arquivos auxiliares em `/extra/auxiliary_files`.

Fique √† vontade para estruturar sub-pastas como quiser l√° dentro. O que importa √© que fique claro o que s√£o esses arquivos.

### 6. (Caso necess√°rio) Criar tabela dicion√°rio

Muitas vezes, especialmente com bases antigas, h√° m√∫ltiplos dicion√°rios em formatos Excel ou outros. Na Base dos Dados n√≥s unificamos tudo em um √∫nico arquivo em formato `.csv` - um √∫nico dicion√°rio para todas as colunas de todas as tabelas do seu conjunto.

!!! Info "Detalhes importantes de como construir seu dicion√°rio est√£o no nosso [manual de estilo](../style_data/#dicionarios)."

#### Exemplo: RAIS - Dicion√°rio

O dicion√°rio completo [pode ser consultado
aqui](https://docs.google.com/spreadsheets/d/12Wwp48ZJVux26rCotx43lzdWmVL54JinsNnLIV3jnyM/edit?usp=sharing).
Ele j√° possui a estrutura padr√£o que utilizamos para dicion√°rios.

### 7. Subir tudo no Google Cloud

Tudo pronto! Agora s√≥ falta subir para o Google Cloud e enviar para
revis√£o. Para isso, vamos usar o cliente `basedosdados` (dispon√≠vel em Python) que facilita as configura√ß√µes e etapas do processo.

!!! Info "Como existe um custo para o armazenamento no storage, para finalizar essa etapa vamos precisar te disponibilizar uma api_key especifica para volunt√°rios para subir os dados em nosso ambiente de desenvolvimento. Assim, entre em nosso [canal no Discord](https://discord.gg/huKWpsVYx4) e nos chame em 'quero-contribuir'"

#### Configure suas credenciais localmente
  **7.1** No seu terminal instale nosso cliente: `pip install basedosdados`.  
  **7.2** Rode `import basedosdados as bd` no python e siga o passo a passo para configurar localmente com as credenciais de seu projeto no Google Cloud. Preencha as informa√ß√µes conforme a seguir:  
```
    * STEP 1: y  
    * STEP 2: basedosdados-dev  (colocar o .json passado pela equipe da bd na pasta credentials)
    * STEP 3: y  
    * STEP 4: basedosdados-dev   
    * STEP 5: https://api.basedosdados.org/api/v1/graphql  
```
#### Suba os arquivos na Cloud
Os dados v√£o passar por 3 lugares no Google Cloud:

  * **Storage**: tamb√©m chamado de GCS √© o local onde ser√£o armazenados o arquivos "frios" (arquiteturas, dados, arquivos auxiliares).
  * **BigQuery-DEV-Staging**: tabela que conecta os dados do storage ao projeto basedosdados-dev no bigquery
  * **BigQuery-DEV-Produ√ß√£o**: tabela utilizada paras teste e tratamento via SQL do conjunto de dados

**7.3** Crie a tabela no *bucket do GCS* e *BigQuey-DEV-staging*, usando a API do Python, da seguinte forma:

    ```python
    import basedosdados as bd
    
    tb = bd.Table(
      dataset_id='<dataset_id>', 
      table_id='<table_id>')

    tb.create(
        path='<caminho_para_os_dados>',
        if_table_exists='raise',
        if_storage_data_exists='raise',
    )
    ```

    Os seguintes par√¢metros podem ser usados:

    
    - `path` (obrigat√≥rio): o caminho completo do arquivo no seu computador, como: `/Users/<seu_usuario>/projetos/basedosdados/mais/bases/[DATASET_ID]/output/microdados.csv`. 
    
    
    !!! Tip "Caso seus dados sejam particionados, o caminho deve apontar para a pasta onde est√£o as parti√ß√µes. No contr√°rio, deve apontar para um arquivo `.csv` (por exemplo, microdados.csv)."
  
    - `force_dataset`: comando que cria os arquivos de configura√ß√£o do dataset no BigQuery.
        - _True_: os arquivos de configura√ß√£o do dataset ser√£o criados no seu projeto e, caso ele n√£o exista no BigQuery, ser√° criado automaticamente. **Se voc√™ j√° tiver criado e configurado o dataset, n√£o use esta op√ß√£o, pois ir√° sobrescrever arquivos**.
        - _False_: o dataset n√£o ser√° recriado e, se n√£o existir, ser√° criado automaticamente.   
    - `if_table_exists` : comando utilizado caso a **tabela j√° exista no BQ**:    
        - _raise_: retorna mensagem de erro.
        - _replace_: substitui a tabela. 
        - _pass_: n√£o faz nada.

    - `if_storage_data_exists`: comando utilizado caso os **dados j√° existam no Google Cloud Storage**:
        - _raise_: retorna mensagem de erro
        - _replace_: substitui os dados existentes.
        - _pass_: n√£o faz nada.

    !!! Info "Se o projeto n√£o existir no BigQuery, ele ser√° automaticamente criado"
    
  Consulte tamb√©m nossa [API](../api_reference_cli) para mais detalhes de cada m√©todo.

**7.4** Crie os arquivos .sql e schema.yml a partir da tabela de arquitetura seguindo essa [documenta√ß√£o](https://github.com/basedosdados/pipelines/wiki/Fun%C3%A7%C3%A3o-%60create_yaml_file()%60)  
!!! Tip "Caso voc√™ precise, nesse momento voc√™ pode alterar a consulta em SQL para realizar tratamentos finais a partir da tabela `staging`, pode incluir coluna, remover coluna, fazer opera√ß√µes alg√©bricas, substituir strings, etc. O SQL √© o limite!"

**7.5** Rode e teste os modelos localmente seguindo essa [documenta√ß√£o](https://github.com/basedosdados/pipelines/wiki/Testar-modelos-dbt-localmente)

**7.6** Suba os metadados da tabela no site:  
!!! Info "Por enquanto apenas a equipe dados tem permiss√µes de subir os metadados da tabela no site, por isso ser√° necess√°rio entrar em contato conosco. J√° estamos trabalhando para que, num futuro pr√≥ximo, os volunt√°rios tamb√©m possam atualizar dados no site."

**7.7** Suba os arquivos auxiliares:  
    ```python
    st = bd.Storage(
      dataset_id = <dataset_id>, 
      table_id = <table_id>)

    st.upload(
      path='caminho_para_os_arquivos_auxiliares',
      mode = 'auxiliary_files',
      if_exists = 'raise')
    ```
    
### 8. Enviar tudo para revis√£o

Ufa, √© isso! Agora s√≥ resta enviar tudo para revis√£o no
[reposit√≥rio](https://github.com/basedosdados/queries-basedosdados) da Base dos Dados.

1. Clone o nosso [reposit√≥rio](https://github.com/basedosdados/queries-basedosdados) localmente.  
2. D√™ um `cd` para a pasta local do reposit√≥rio e abra uma nova branch com `git checkout -b [dataset_id]`. Todas as adi√ß√µes e modifica√ß√µes ser√£o inclu√≠das nessa _branch_.  
3. Para cada tabela nova incluir o arquivo com nome `table_id.sql` na pasta `queries-basedosdados/models/dataset_id/` copiando as queries que voc√™ desenvolveu no passo 7.  
4. Incluir o arquivo schema.yaml desenvolvido no passo 7  
5. Caso seja um dataset novo, incluir o dataset conforme as instru√ß√µes do arquivo `queries-basedosdados/dbt_project.yaml` (n√£o se esque√ßa de seguir a ordem alfab√©tica pra n√£o bagun√ßar a organiza√ß√£o)  
6. Inclua o seu c√≥digo de captura e limpeza em na pasta `queries-basedosdados/models/dataset_id/code`  
7. Agora √© s√≥ publicar a branch, abrir o PR com as labels 'table-approve' e marcar a equipe dados para corre√ß√£o  

**E agora?** Nossa equipe ir√° revisar os dados e metadados submetidos
via Github. Podemos entrar em contato para tirar d√∫vidas ou solicitar
mudan√ßas no c√≥digo. Quando tudo estiver OK, fazemos um _merge_ do seu
*pull request* e os dados s√£o automaticamente publicados na nossa
plataforma!
