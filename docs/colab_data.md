

# Suba dados na BD+

## Por que minha organiza√ß√£o deve subir dados na BD+?

- **Capacidade de cruzar suas bases com dados de diferentes
  organiza√ß√µes** de forma simples e f√°cil. J√° s√£o centenas de conjuntos
  de dados p√∫blicos das maiores organiza√ß√µes do Brasil e do mundo presentes
  no nosso *datalake*.

- **Compromisso com a transpar√™ncia, qualidade dos dados e
  desenvolvimento de melhores pesquisas, an√°lises e solu√ß√µes** para a
  sociedade. N√£o s√≥ democratizamos o acesso a dados abertos, mas tamb√©m dados
  de qualidade. Temos um time especializado que revisa e garante a qualidade dos
  dados adicionados ao *datalake*.

- **Participa√ß√£o de uma comunidade que cresce cada vez mais**: milhares
  de jornalistas, pesquisadores(as), desenvolvedores(as), j√° utilizam e
  acompanham a Base dos Dados.
  <!-- TODO: Colocar aqui o link do nosso painel de audiencia quando tiver pronto :) -->

## Passo a passo para subir dados

Quer subir dados na BD+ e nos ajudar a construir esse reposit√≥rio?
*Maravilha!* Organizamos tudo o que voc√™ precisa no manual abaixo

Para facilitar a explica√ß√£o, vamos seguir um exemplo j√° pronto com dados da [RAIS](https://basedosdados.org/dataset/br-me-rais).

!!! Tip "Voc√™ pode navegar pelas etapas no menu √† esquerda."
    Sugerimos fortemente que entre em nosso [canal no
    Discord](https://discord.gg/huKWpsVYx4) para tirar d√∫vidas e
    interagir com a equipe e outros(as) colaboradores(as)! üòâ

### Antes de come√ßar

Alguns conhecimentos s√£o necess√°rias para realizar esse processo:

- **Python, R, SQL e/ou Stata**: para criar os c√≥digos de captura e limpeza dos dados.
- **Linha de comando**: para configurar seu ambiente local
  e conex√£o com o Google Cloud.
- **Github**: para subir seu c√≥digo para revis√£o da
  nossa equipe.

!!! Tip "N√£o tem alguma dessas habilidades, mas quer colaborar?"
    Temos um time de dados que pode te ajudar, basta entrar no [nosso
    Discord](https://discord.gg/huKWpsVYx4) e mandar uma mensagem em #quero-contribuir.

### Como funciona o processo?

* Iniciamos entendendo a natureza dos dados, precisamos conhecer o que estamos disponibilizando
* Montamos uma arquitetura que vai indicar como que a nossa tabela final ficar√°
* Realizamos o tratamento nos dados. Ele pode ser feito em python, R, Stata ou SQL 
* Subimos dados no datalake
* Realizamos checks de qualidade nos dados
* Levamos os dados para produ√ß√£o

### 1. Informar seu interesse para a gente

Mantemos a lista de conjuntos f√°ceis de come√ßar a trabalhar no nosso [Github](https://github.com/orgs/basedosdados/projects/17/views/10). Para come√ßar a subir uma base do seu interesse, basta abrir uma [nova issue](https://github.com/basedosdados/mais/issues/new?assignees=&labels=data&template=br_novos_dados.md&title=%5Bdados%5D+%3Cdataset_id%3E) de dados e preencher as informa√ß√µes indicadas por l√°.

!!! Info "Caso sua base (conjunto) j√° esteja listada, basta marcar seu usu√°rio do Github como `assignee`."


### 2. Baixar nossa pasta _template_ para dados

[Baixe aqui a pasta
_template_](https://drive.google.com/drive/folders/1xXXon0vdjSKr8RCNcymRdOKgq64iqfS5?usp=sharing)
 e renomeie para o nome do seu conjunto de dados, `<dataset_id>`
([veja aqui como nomear seu conjunto](../style_data)). Ela facilita todos os
passos daqui pra frente. Sua
estrutura √© a seguinte:

- `<dataset_id>/`
    - `code/`: C√≥digos necess√°rios para **captura** e **limpeza** dos dados
    ([veja no passo
    5](#5-escrever-codigo-de-captura-e-limpeza-de-dados)).
    - `input/`: Cont√©m todos os arquivos com dados originais, exatamente
    como baixados da fonte prim√°ria. **Esses arquivos n√£o devem ser
    modificados.**
    - `output/`: Arquivos finais, j√° no formato pronto para subir na BD+.
    - `tmp/`: Quaisquer arquivos tempor√°rios criados pelo c√≥digo em `/code` no processo de limpeza e tratamento.
    - `extra/`
        - `architecture/`: Tabelas de arquitetura ([veja no passo 4](#4-preencher-as-tabelas-de-arquitetura)).
        - `auxiliary_files/`: Arquivos auxiliares aos dados ([veja no passo 6](#6-caso-necessario-organizar-arquivos-auxiliares)).
        - `dicionario.csv`: Tabela dicion√°rio de todo o conjunto de dados ([veja no passo
          7](#7-caso-necessario-criar-tabela-dicionario)).

!!! info "As pastas `input`, `output` e `tmp` n√£o ser√£o commitadas para o seu projeto e existir√£o apenas localmente."

### 3. Preencher as tabelas de arquitetura

As tabelas de arquitetura determinam **qual a estrutura de
cada tabela do seu conjunto de dados**. Elas definem, por exemplo, o nome, ordem e metadados das colunas, e
como uma coluna deve ser tratada quando h√° mudan√ßas em vers√µes (por
exemplo, se uma coluna muda de nome de um ano para o outro).

!!! Info "Cada tabela do conjunto de dados deve ter sua pr√≥pria tabela de arquitetura (planilha), que pode ser preenchida no Google Drive ou localmente (Excel, editor de texto)."

Para cada informa√ß√£o que for preencher na tabela de arquitetura consulte nosso [manual de estilo](../style_data) para manter a padroniza√ß√£o.

- `name`: nome da coluna padronizada

- `bigquery_type`: tipo de dado do BigQuery (veja quais s√£o no nosso [manual de estilo](../style_data/#tipos-de-variaveis)).

- `description`: descri√ß√£o mais detalhada da vari√°vel.

- `temporal_coverage`: cobertura temporal da vari√°vel nessa tabela (veja como preencher no nosso [manual de estilo](https://basedosdados.github.io/mais/style_data/#cobertura-temporal)).

- `covered_by_dictionary`: indicar se a vari√°vel √© coberta por dicion√°rio. **Op√ß√µes de respostas s√£o: `yes` e `no`**.

- `directory_column`: se a coluna for coberta por um diret√≥rio da BD, usar o formato [DATASET_ID].[TABLE]:[COLUNA] (e.g. `br_bd_diretorios_data_tempo.ano:ano`). Caso contr√°rio, deixe em branco

- `measurement_unit`: qual unidade de medida da coluna (veja as unidades de medida dispon√≠veis para preenchimento no nosso [Github](https://github.com/basedosdados/website/blob/master/ckanext-basedosdados/ckanext/basedosdados/validator/available_options/measurement_unit.py).

- `has_sensitive_data`: indicar se a coluna possui dados sens√≠veis (e.g. CPF identificado, dados de conta banc√°ria, etc). 
**Op√ß√µes de preenchimento s√£o: yes, no**.

- `observations`: observa√ß√µes de tratamento que precisam ser evidenciados. Indicar, por exemplo, porque determinada coluna foi criada ou modificada.

- `original_name`: indicar o nome original de cada coluna para cada ano, no formato `original_name_YYYY`. No exemplo da RAIS, existiam colunas que deixaram de existir em determinados anos. Por isso, criamos colunas √† direita  em ordem descendente (e.g. 2020, 2019, 2018, ...).

<!-- 
TODO: N√£o vejo rela√ß√£o dessas perguntas com o exemplo dado
da tabela de arquitetura da RAIS.
Perguntas que uma arquitetura deve responder:
* *Quais ser√£o as tabelas finais na base?* Essas n√£o precisam ser
  exatamente o que veio nos dados brutos. -> n√£o tem isso no exemplo
* *Qual √© n√≠vel da observa√ß√£o de cada tabela?* O "n√≠vel da observa√ß√£o" √©
  a menor unidade a que se refere cada linha na tabela (ex: municipio-ano, candidato,
  estabelecimento-cnae). -> n√£o tem isso no exemplo
* *Ser√° gerada alguma tabela derivada com agrega√ß√µes em cima dos dados
  originais?* -> n√£o tem isso no exemplo -->

#### Exemplo: RAIS - Tabelas de arquitetura

As tabelas de arquitetura preenchidas [podem ser consultadas aqui](https://docs.google.com/spreadsheets/d/1dPLUCeE4MSjs0ykYUDsFd-e7-9Nk6LVV/edit?usp=sharing&ouid=103008455637924805982&rtpof=true&sd=true).
Nessa tabela n√≥s:
- renomeamos as colunas
- definimos os tipos
- preenchemos descri√ß√µes 
- indicamos se h√° dicion√°rio ou diret√≥rio 
- varificamos a cobertura temporal de cada coluna 
- incluimos a unidade de medida
- fizemos a compatibiliza√ß√£o entre anos para todas as vari√°veis (colunas)

Para as que foram exclu√≠das da vers√£o em produ√ß√£o,
deixamos seu nome vazio ou `(deletado)` e n√£o preenchemos nenhum metadado.
Por exemplo, a coluna `Tipo Estab` da tabela `microdados_vinculos` n√£o
foi adicionada. Logo, ela aparece com o nome
`(deletado)` na respectiva tabela de arquitetura (√∫ltima linha).

!!! Tip "Quando terminar de preencher as tabelas de arquitetura, entre em contato com a equipe da Base dos Dados ou nossa comunidade para validar tudo. √â importante ter certeza que est√° padronizado _antes_ de come√ßar a escrever c√≥digo."

### 4. Escrever c√≥digo de captura e limpeza de dados

Ap√≥s validadas as tabelas de arquitetura, podemos escrever os c√≥digos de
**captura** e **limpeza** dos dados.

- **Captura**: C√≥digo que baixa automaticamente todos os dados originais e os salva em `/input`. Esses dados podem estar dispon√≠veis em portais ou links FTP, podem ser raspados de sites, entre outros.

- **Limpeza**: C√≥digo que transforma os dados originais salvos em `/input` em dados limpos, salva na pasta `/output`, para, posteriormente, serem subidos na BD+.

Cada tabela limpa para produ√ß√£o pode ser salva como um arquivo √∫nico ou, caso seja muito grande (e.g. acima de 100-200 mb), ser particionada no formato [Hive](https://cloud.google.com/bigquery/docs/hive-partitioned-loads-gcs) em v√°rios sub-arquivos. Os formatos aceitos s√£o `.csv` ou `.parquet`. Nossa recomenda√ß√£o √© particionar tabelas por `ano`, `mes`, `sigla_uf` ou no m√°ximo por `id_municipio`. 

#### Exemplo: RAIS
A tabela `microdados_vinculos` da RAIS, por exemplo, √© uma tabela muito grande (+250GB) por isso n√≥s particionamos por `ano` e `sigla_uf`. O particionamento foi feito usando a estrutura de pastas `/microdados_vinculos/ano=YYYY/sigla_uf=XX`.

!!! Tip "No pacote `basedosdados` voc√™ encontra fun√ß√µes √∫teis para limpeza dos dados"
    Definimos fun√ß√µes para particionar tabelas de forma autom√°tica,
    criar vari√°veis comuns (e.g. `sigla_uf` a partir de `id_uf`) e
    outras. Veja em [Python](../api_reference_python) e [R](../api_reference_r)

#### Padr√µes necess√°rios no c√≥digo

- Devem ser escritos em [Python](https://www.python.org/),
  [R](https://www.r-project.org/) ou [Stata](https://www.stata.com/) -
  para que a revis√£o possa ser realizada pela equipe.
- Pode estar em script (`.py`, `.R`, ...) ou *notebooks* (Google Colab, Jupyter, Rmarkdown, etc).
- Os caminhos de arquivos devem ser atalhos _relativos_ √† pasta ra√≠z
  (`<dataset_id>`), ou seja, n√£o devem depender dos caminhos do seu
  computador.
- A limpeza deve seguir nosso [manual de estilo](../style_data) e as [melhores pr√°ticas de programa√ß√£o](https://en.wikipedia.org/wiki/Best_coding_practices).

#### Exemplo: PNAD Cont√≠nua - C√≥digo de limpeza

O c√≥digo de limpeza foi constru√≠do em R e [pode ser consultado
aqui](https://github.com/basedosdados/mais/tree/master/bases/br_ibge_pnadc/code).


### 5. (Caso necess√°rio) Organizar arquivos auxiliares

√â comum bases de dados serem disponibilizadas com arquivos auxiliares. Esses podem incluir notas t√©cnicas, descri√ß√µes de coleta e amostragem, etc. Para ajudar usu√°rios da Base dos Dados terem mais contexto e entenderem melhor os dados, organize todos esses arquivos auxiliares em `/extra/auxiliary_files`.

Fique √† vontade para estruturar sub-pastas como quiser l√° dentro. O que importa √© que fique claro o que s√£o esses arquivos.

### 6. (Caso necess√°rio) Criar tabela dicion√°rio

Muitas vezes, especialmente com bases antigas, h√° m√∫ltiplos dicion√°rios
em formatos Excel ou outros. Na Base dos Dados n√≥s unificamos tudo em um
√∫nico arquivo em formato `.csv` - um √∫nico dicion√°rio para todas as
colunas de todas as tabelas do seu conjunto.


!!! Info "Detalhes importantes de como construir seu dicion√°rio est√£o no nosso [manual de estilo](../style_data/#dicionarios)."

#### Exemplo: RAIS - Dicion√°rio

O dicion√°rio completo [pode ser consultado
aqui](https://docs.google.com/spreadsheets/d/12Wwp48ZJVux26rCotx43lzdWmVL54JinsNnLIV3jnyM/edit?usp=sharing).
Ele j√° possui a estrutura padr√£o que utilizamos para dicion√°rios.

### 7. Configure suas credenciais localmente e prepare o ambiente

No seu terminal:

1. Instale nosso cliente: `pip install basedosdados`.
2. Rode `basedosdados config init` e siga o passo a passo para configurar localmente com as credenciais de seu projeto no Google Cloud.

> Caso seu ambiente de produ√ß√£o n√£o permita o uso interativo do nosso cliente ou apresente alguma outra dificuldade relativa a esse modo de configura√ß√£o, voc√™ pode configurar o `basedosdados` a partir de vari√°veis de ambiente da seguinte forma:
>
>```bash
> export BASEDOSDADOS_CONFIG=$(cat ~/.basedosdados/config.toml | base64)
> export BASEDOSDADOS_CREDENTIALS_PROD=$(cat ~/.basedosdados/credentials/prod.json | base64)
> export BASEDOSDADOS_CREDENTIALS_STAGING=$(cat ~/.basedosdados/credentials/staging.json | base64)
>```
>
3. Clone um _fork_ do nosso [reposit√≥rio](https://github.com/basedosdados/mais) localmente.
4. D√™ um `cd` para a pasta local do reposit√≥rio e abra uma nova branch
   com `git checkout -b [BRANCH_ID]`. Todas as adi√ß√µes e modifica√ß√µes
   ser√£o inclu√≠das nessa _branch_.

### 8. Subir tudo no Google Cloud

Tudo pronto! Agora s√≥ falta subir para o Google Cloud e enviar para
revis√£o. Para isso, vamos usar o cliente `basedosdados` (dispon√≠vel em
linha de comando e Python) que facilita as configura√ß√µes e etapas do processo.

#### Configure seu projeto no Google Cloud e um _bucket_ no Google Storage

Os dados v√£o passar ao todo por 3 lugares no Google Cloud:

1. **Storage**: local onde ser√£o armazenados o arquivos "frios" (arquiteturas, dados, arquivos auxiliares).
2. **BigQuery**: super banco de dados do Google, dividido em 2 projetos/tipos de tabela:
    - *Staging*: banco para teste e tratamento final do conjunto de dados.
    - *Produ√ß√£o*: banco oficial de publica√ß√£o dos dados (nosso projeto `basedosdados` ou o seu mesmo caso queira reproduzir o ambiente)

√â necess√°rio ter uma conta Google e um projeto (gratuito) no Google
Cloud. Para criar seu projeto basta:

1. Acessar o [link](https://console.cloud.google.com/projectselector2/home/dashboard) e aceitar o Termo de Servi√ßos do Google Cloud.
2. Clicar em `Create Project/Criar Projeto` - escolha um nome bacana para o seu projeto, ele ter√° tamb√©m um `Project ID` que ser√° utilizado para configura√ß√£o local.
3. Depois de criado o projeto, v√° at√© a funcionalidade de
   [Storage](https://console.cloud.google.com/storage) e crie uma pasta
   (_bucket_) para subir os dados (pode ter o mesmo nome do projeto).



#### Suba e configure uma tabela no seu _bucket_

Aqui s√£o dois passos: primeiro publicamos uma base (conjunto) e depois publicamos tabelas. Este processo
est√° automatizado na cria√ß√£o das tabelas, conforme abaixo.

<!--

Para publicar uma **base (conjunto)**:

1. Inicialize o conjunto no seu reposit√≥rio local:

    ```bash
   basedosdados dataset init [DATASET_ID]
    ```

4. Crie o conjunto no *BigQuery* rodando:

    ```bash
    basedosdados dataset create [DATASET_ID]
    ```

5. Preencha os arquivos de configura√ß√£o criados em `<dataset_id>/`:
    - `README.md`: informa√ß√µes √∫teis para quem for ver o c√≥digo no Github.
    - `dataset_config.yaml`: informa√ß√µes espec√≠ficas do conjunto (j√° vem
      com um modelo para preenchimento).
6. Atualize as informa√ß√µes preenchidas do conjunto com:

    ```bash
    basedosdados dataset update [DATASET_ID]
    ```
-->

Para publicar o dataset e a(s) **tabela(s)**:

1. Crie a tabela no *bucket*, usando a API do Python, da seguinte forma:

    ```python
    import basedosdados as bd
    
    tb = bd.Table(dataset_id=<dataset_id>, table_id=<table_id>)
    tb.create(
        path='caminho_para_os_dados',
        if_table_exists='raise',
        if_storage_data_exists='raise',
    )
    ```

    Os seguintes par√¢metros podem ser usados (no terminal ou no Python):

    
    - `-- path` (obrigat√≥rio): o caminho completo do arquivo no seu computador, como: `/Users/<seu_usuario>/projetos/basedosdados/mais/bases/[DATASET_ID]/output/microdados.csv`. Caso seus dados sejam particionados, o caminho deve apontar para a pasta onde est√£o as parti√ß√µes. No contr√°rio, deve apontar para um arquivo `.csv` (por exemplo, microdados.csv).
  
    - `--force_dataset` [True|False]: comando que cria os arquivos de configura√ß√£o do dataset locais e no BigQuery.
        - _True_: os arquivos de configura√ß√£o do dataset ser√£o criados no seu projeto e, caso ele n√£o exista no BigQuery, ser√° criado automaticamente. Se voc√™ j√° tiver criado e configurado o dataset. **Se voc√™ j√° tiver criado e configurado o dataset, n√£o use esta op√ß√£o, pois ir√° sobrescrever arquivos**.
        
        - _False_: o dataset n√£o ser√° recriado e, se n√£o existir, ser√° criado automaticamente.
  
   
    - `--if_table_exists` [raise|replace|pass]: comando utilizado quando a **tabela j√° existe**:    
        - _raise_: retorna mensagem de erroo.
        - _replace_: substitui a tabela. 
        - _pass_: n√£o faz nada.

    - `--if_storage_data_exists` [raise|replace|pass]: comando utilizado quando os **dados j√° existem** no Google Cloud Storage.
        - _raise_: retorna mensagem de erro
        - _replace_: substitui os dados existentes.
        - _pass_: n√£o faz nada.
   
    - `--if_table_config_exists` [raise|replace|pass]: comando utilizado quando as **configura√ß√µes da tabela j√° existem**.
        - _raise_: retorna mensagem de erro
        - _replace_: substitui as configura√ß√µes da tabela existentes
        - _pass_: n√£o faz nada
   
    - `--columns_config_url` [google sheets url]: comando para preencher os metadados via arquitetura constru√≠da do Google Sheets. A url precisa estar no formato _https://docs.google.com/spreadsheets/d/<table_key> edit#gid=<table_gid>_.
    Certifique-se de que a planilha est√° compartilhada com a op√ß√£o "qualquer pessoa com o link pode ver"


!!! Info "Se o projeto n√£o existir no BigQuery, ele ser√° automaticamente criado"

2. Preencha os arquivos de configura√ß√£o da tabela:

- `/[TABLE_ID]/publish.sql`: aqui voc√™ pode indicar tratamentos finais
    na tabela `staging` em SQL para publica√ß√£o (e.g. modificar a query para
    dar um `JOIN` em outra tabela da BD+ e selecionar vari√°veis).

3. Publique a tabela em produ√ß√£o :

    ```bash
    basedosdados table publish [DATASET_ID] [TABLE_ID]
    ```
   Se os metadados de colunas j√° tiverem sido preenchidos no site o pacote j√° vai montar um arquivo `.sql` com os tipos do BigQuery corretos.

Consulte tamb√©m nossa [API](../api_reference_cli) para mais detalhes de cada m√©todo.

!!! Tip "Abra o console do BigQuery e rode algumas _queries_ para testar se foi tudo publicado corretamente. Estamos desenvolvendo testes autom√°ticos para facilitar esse processo no futuro."

### 9. Validar os metadados para publica√ß√£o

Para validar os metadados preenchidos nos arquivos `dataset_config.yaml` e `table_config.yaml`, o processo √© simples.

<!--
1. Coloque suas credenciais do CKAN no arquivo
   `~/.basedosdados/config.toml`, preenchendo os campos `api_key` e
   `url` em `[ckan]`. Este arquivo √© criado automaticamente ao rodar
   `basedosdados config init` [(veja no passo
   7)](#7-subir-tudo-no-google-cloud).
-->
1. Valide os metadados do conjunto atrav√©s do comando `basedosdados metadata validate [DATASET_ID]`. Para validar metadados de tabelas, basta rodar `basedosdados metadata validate [DATASET_ID] [TABLE_ID]`.
2. Ap√≥s a revis√£o da nossa equipe, os dados ser√£o publicados no CKAN.
<!--
3. Publique os metadados do conjunto j√° preenchidos e validados com
   `basedosdados metadata publish [DATASET_ID]`. Para publicar metadados
   de tabelas, use `basedosdados metadata publish [DATASET_ID]
   [TABLE_ID]`.

> Para publicar metadados de ambos (conjunto e tabela), use
> `basedosdados metadata publish [DATASET_ID] [TABLE_ID] --all=True`.
-->

#### Atualizando metadados de bases ou tabelas j√° existentes

Atrav√©s do m√≥dulo `metadata` √© poss√≠vel tamb√©m trabalhar com bases e tabelas j√° existentes na plataforma. Elas podem ser atualizadas a partir do procedimento que descrevemos a seguir.

1. Rode `basedosdados metadata create [DATASET_ID]` para puxar os
   metadados do conjunto dispon√≠veis no site para o arquivo
   `dataset_config.yaml`. Para puxar metadados de tabelas, use
   `basedosdados metadata create [DATASET_ID] [TABLE_ID]`, que ir√°
   atualizo arquivo `table_config.yaml`
2. Preencha os novos valores de metadados nos respectivos arquivos.
3. Rode `basedosdados metadata validate [DATASET_ID]` <!-- e para publicar os
   metadados atualizados do conjutno use `basedosdados metadata publish [DATASET_ID]`. O mesmo pode ser feito
   para tabela adicionando o parametro `[TABLE_ID]` -->

### 10. Enviar tudo para revis√£o

Ufa, √© isso! Agora s√≥ resta enviar tudo para revis√£o no
[reposit√≥rio](https://github.com/basedosdados/mais) da Base dos Dados.
Crie os _commits_ necess√°rios e rode `git push origin [BRANCH_ID]`.
Depois √© s√≥ abrir um _pull request_ (PR) no nosso reposit√≥rio.

**E agora?** Nossa equipe ir√° revisar os dados e metadados submetidos
via Github. Podemos entrar em contato para tirar d√∫vidas ou solicitar
mudan√ßas no c√≥digo. Quando tudo estiver OK, fazemos um _merge_ do seu
*pull request* e os dados s√£o automaticamente publicados na nossa
plataforma!
