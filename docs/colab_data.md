
# Suba dados na BD+

## Por que minha organiza√ß√£o deve subir dados na BD+?

- **Capacidade de cruzar suas bases com dados de diferentes
  organiza√ß√µes** de forma simples e f√°cil. J√° s√£o centenas de conjuntos
  dados p√∫blicos das maiores organiza√ß√µes do Brasil e do mundo presentes
  no nossod *datalake*.

- **Compromisso com a transpar√™ncia, qualidade dos dados e
  desenvolvimento de melhores pesquisas, an√°lises e solu√ß√µes** para a
  sociedade. N√£o s√≥ democratizamos o acesso a dados abertos, mas dados
  de qualidade. Temos um time especializado que revisa e garante a qualidade dos
  dados adicionados ao *datalake*.

- **Participa√ß√£o de uma comunidade que cresce cada vez mais**: milhares
  de jornalistas, pesquisadores(as), desenvolvedores(as), j√° utilizam e
  acompanham a Base dos Dados.
  <!-- TODO: Colocar aqui o link do nosso painel de audiencia quando tiver pronto :) -->

## Passo a passo para subir dados

Quer subir dados na BD+ e nos ajudar a construir esse reposit√≥rio?
*Maravilha!* Organizamos tudo o que voc√™ precisa no manual abaixo, em 10 passos.

Para facilitar a explica√ß√£o, vamos seguir um exemplo j√° pronto com dados da [RAIS](https://basedosdados.org/dataset/br-me-rais).

!!! Tip "Voc√™ pode navegar pelas etapas no menu √† esquerda."
    Sugerimos fortemente que entre em nosso [canal no
    Discord](https://discord.gg/huKWpsVYx4) para tirar d√∫vidas e
    interagir com a equipe e outros(as) colaboradores(as)! üòâ

### Antes de come√ßar

Alguns conhecimentos s√£o necess√°rias para realizar esse processo:

- **Python, R, SQL e/ou Stata**, para criar os c√≥digos de captura e limpeza dos dados.
- **Linha de comando**, para configurar seu ambiente local
  e conex√£o com o Google Cloud.
- **Github**, para subir seu c√≥digo para revis√£o da
  nossa equipe.

!!! Tip "N√£o tem alguma dessas habilidades mas quer colaborar?"
    Temos um time de dados que pode te ajudar, basta entrar no [nosso
    Discord](https://discord.gg/huKWpsVYx4) e mandar uma mensagem em #quero-contribuir.

### 1. Informar seu interesse para a gente

Mantemos a lista de conjuntos que ainda n√£o est√£o na BD+ no nosso [Github](https://github.com/basedosdados/mais/issues?q=is%3Aopen+is%3Aissue+label%3Adata+label%3Aenhancement). Para come√ßar a subir uma base do seu interesse, basta abrir um [novo issue](https://github.com/basedosdados/mais/issues/new?assignees=&labels=data&template=br_novos_dados.md&title=%5Bdados%5D+%3Cdataset_id%3E) de dados e preencher as informa√ß√µes indicadas por l√°.

!!! Info "Caso sua base (conjunto) j√° esteja listada, basta marcar seu usu√°rio do Github como `assignee`."

### 2. Configure suas credenciais localmente e prepare o ambiente

No seu terminal:

1. Instale nosso cliente: `pip install basedosdados`.
2. Rode `basedosdados config init` e siga o passo a passo para configurar localmente com as credenciais de seu projeto no Google Cloud.

> Caso seu ambiente de produ√ß√£o n√£o permita o uso interativo do nosso cliente ou apresente alguma outra dificuldade relativa a esse modo de configura√ß√£o, voc√™ pode configurar o `basedosdados` a partir de vari√°veis de ambiente da seguinte forma:
>
>```bash
> export BASEDOSDADOS_CONFIG=$(cat ~/.basedosdados/config.toml | base64)
> export BASEDOSDADOS_CREDENTIALS_PROD=$(cat ~/.basedosdados/credentials/prod.json | base64)
> export BASEDOSDADOS_CREDENTIALS_STAGING=$(cat ~/.basedosdados/credentials/staging.json | base64)
>```
>
3. Clone um _fork_ do nosso [reposit√≥rio](https://github.com/basedosdados/mais) localmente.
4. D√™ um `cd` para a pasta local do reposit√≥rio e abra uma nova branch
   com `git checkout -b [BRANCH_ID]`. Todas as adi√ß√µes e modifica√ß√µes
   ser√£o feitas nessa _branch_.

### 3. Baixar nossa pasta _template_ para dados

[Baixe aqui a pasta
_template_](https://drive.google.com/drive/folders/1xXXon0vdjSKr8RCNcymRdOKgq64iqfS5?usp=sharing)
para a pasta ``dados`` e renomeie para o nome do seu conjunto de dados, `<dataset_id>`
([veja aqui como nomear seu conjunto](../style_data)). Ela facilita todos os
passos daqui pra frente. Sua
estrutura √© a seguinte:

- `<dataset_id>/`
    - `code/`: C√≥digos necess√°rios √† **captura** e **limpeza** dos dados
    ([veja no passo
    5](#5-escrever-codigo-de-captura-e-limpeza-de-dados)).
    - `input/`: Cont√©m todos os arquivos com dados originais, exatamente
    como baixados da fonte prim√°ria **Esses arquivos n√£o devem ser
    modificados.**
    - `output/`: Arquivos finais, j√° no formato pronto para subir na BD+.
    - `tmp/`: Quaisquer arquivos tempor√°rios criados pelo c√≥digo em `/code` no processo de limpeza e tratamento.
    - `extra/`
        - `architecture/`: Tabelas de arquitetura ([veja no passo 4](#4-preencher-as-tabelas-de-arquitetura)).
        - `auxiliary_files/`: Arquivos auxiliares aos dados ([veja no passo 6](#6-caso-necessario-organizar-arquivos-auxiliares)).
        - `dicionario.csv`: Tabela dicion√°rio de todo o conjunto de dados ([veja no passo
          7](#7-caso-necessario-criar-tabela-dicionario)).

!!! info "As pastas input, output e tmp n√£o ser√£o commitadas para o seu projeto e existir√£o apenas localmente"

### 4. Preencher as tabelas de arquitetura

As tabelas de arquitetura determinam **qual a estrutura de
cada tabela do seu conjunto de dados**. Elas definem, por exemplo, o nome, ordem e metadados das colunas, e
como uma coluna deve ser tratada quando h√° mudan√ßas em vers√µes (por
exemplo, e uma coluna muda de nome de um ano para o outro).

!!! Info "Cada tabela do conjunto de dados deve ter sua pr√≥pria tabela de arquitetura (planilha), que pode ser preenchida no Google Drive ou localmente (Excel, editor de texto)."

<!-- 
TODO: N√£o vejo rela√ß√£o dessas perguntas com o exemplo dado
da tabela de arquitetura da RAIS.
Perguntas que uma arquitetura deve responder:
* *Quais ser√£o as tabelas finais na base?* Essas n√£o precisam ser
  exatamente o que veio nos dados brutos. -> n√£o tem isso no exemplo
* *Qual √© n√≠vel da observa√ß√£o de cada tabela?* O "n√≠vel da observa√ß√£o" √©
  a menor unidade a que se refere cada linha na tabela (ex: municipio-ano, candidato,
  estabelecimento-cnae). -> n√£o tem isso no exemplo
* *Ser√° gerada alguma tabela derivada com agrega√ß√µes em cima dos dados
  originais?* -> n√£o tem isso no exemplo -->

#### Exemplo: RAIS - Tabelas de arquitetura

As tabelas de arquitetura preenchidas [podem ser consultadas aqui](https://drive.google.com/drive/folders/1OtsucP_KhiUEJI6F6k_cagvXfwZCFZF2?usp=sharing). Seguindo nosso [manual de estilo](../style_data), n√≥s renomeamos, definimos o tipo, preenchemos descri√ß√µes, indicamos se h√° dicion√°rio ou diret√≥rio, preenchemos campos (e.g. cobertura temporal e unidade de medida) e fizemos a compatibiliza√ß√£o entre anos para todas as vari√°veis (colunas).

- **nome:** Nome da coluna
- **tipo:** tipo de dado do BigQuery (veja quais s√£o no nosso [manual de estilo](../style_data/#tipos-de-variaveis))
- **descricao:** Descri√ß√£o dos dados que est√£o nesta coluna 
- **unidade_medida:** em qual unidade (no caso de vari√°veis num√©ricas) os dados est√£o
- **dicionario:** podem ser 3 op√ß√µes
    - diretorio: caso se refira a uma tabela de diret√≥rio da BD
    - local: caso se refira a um dicionario no dataset
    - se n√£o houver um dicion√°rio, deixar em branco
- **nome_diretorio:** se a coluna for coberta por um dicion√°rio da BD, usar o formato [DATASET_ID].[TABLE]:[COLUNA]. Caso contr√°rio, deixe em branco

<!-- nos exemplos os nomes de colunas est√£o diferentes do que o pacote Python testa como obrigat√≥rias -->

Nas tabelas desse conjunto existiam colunas que deixaram de existir em
determinados anos. Por isso, foi necess√°rio compatibilizar:
criamos colunas √† direita chamadas `nome_original_YYYY`, em ordem descendente (e.g. 2022,
2021, 2020, ...). Nessas colunas inclu√≠mos _todas_ as vari√°veis de cada
ano.

Para as que foram exclu√≠das da vers√£o em produ√ß√£o,
deixamos seu nome como `(deletado)` e n√£o preenchemos nenhum metadado.
Por exemplo, a coluna `Municipio` da tabela `microdados_vinculos` n√£o
foi adicionada pois por padr√£o usamos somente o c√≥digo IBGE para
identificar munic√≠pios (seu nome fica numa tabela de
[Diret√≥rios](../style_data/#diretorios)). Logo, ela aparece com o nome
`(deletado)` na respectiva tabela de arquitetura (pen√∫ltima linha).

!!! Tip "Quando terminar de preencher as tabelas de arquitetura, entre em contato com a equipe da Base dos Dados ou nossa comunidade para validar tudo. √â importante ter certeza que est√° fazendo sentido _antes_ de come√ßar a escrever c√≥digo."

### 5. Escrever c√≥digo de captura e limpeza de dados

Ap√≥s validadas as tabelas de arquitetura, podemos escrever os c√≥digos de
**captura** e **limpeza** dos dados.

- **Captura**: C√≥digo que baixa automaticamente todos os dados originais e os salva em `/input`. Esses dados podem estar dispon√≠veis em portais ou links FTP, podem ser raspados de sites, entre outros.

- **Limpeza**: C√≥digo que transforma os dados originais salvos em `/input` nos dados limpos prontos para serem subidos na BD+ salvos em `/output`, tudo baseado nas tabelas de arquitetura.

Cada tabela limpa para produ√ß√£o pode ser salva como um arquivo `.csv` √∫nico ou, caso seja muito grande (e.g. acima de 100-200 mb), ser particionada no formato [Hive](https://cloud.google.com/bigquery/docs/hive-partitioned-loads-gcs) em v√°rios sub-arquivos `.csv`. Nossa recomenda√ß√£o √© particionar tabelas por `ano`, `mes`, `sigla_uf`, ou no m√°ximo por `id_municipio`.

!!! Tip "No pacote `basedosdados` voc√™ encontra fun√ß√µes √∫teis para limpeza dos dados"
    Definimos fun√ß√µes para particionar tabelas de forma autom√°tica,
    criar vari√°veis comuns (e.g. `sigla_uf` a partir de `id_uf`) e
    outras. Veja em [Python](../api_reference_python) e [R](../api_reference_r)

#### Padr√µes necess√°rios no c√≥digo

- Devem ser escritos em [Python](https://www.python.org/),
  [R](https://www.r-project.org/), ou [Stata](https://www.stata.com/) -
  para que a revis√£o possa ser realizada pela equipe.
- Pode estar em script (`.py`, `.r`, ...) ou *notebooks* (Google Colab, Jupyter, Rmarkdown, etc).
- Os caminhos de arquivos devem ser atalhos _relativos_ √† pasta ra√≠z
  (`<dataset_id>`), ou seja, n√£o devem depender dos caminhos do seu
  computador.
- A limpeza deve seguir nosso [manual de estilo](../style_data) e as [melhores pr√°ticas de programa√ß√£o](https://en.wikipedia.org/wiki/Best_coding_practices).

#### Exemplo: RAIS - C√≥digo de limpeza

O c√≥digo de limpeza foi constru√≠do em Stata e [pode ser consultado
aqui](https://github.com/basedosdados/mais/tree/master/bases/br_me_rais/code).

A tabela `microdados_vinculos` da RAIS √© uma tabela muito grande
(+250GB) por isso n√≥s particionamos por `ano` e `sigla_uf`. O
particionamento foi feito usando a estrutura de pastas
`/microdados_vinculos/ano=YYYY/sigla_uf=XX`.

### 6. (Caso necess√°rio) Organizar arquivos auxiliares

√â comum bases de dados serem distribu√≠das com arquivos auxiliares. Esses podem incluir notas t√©cnicas, descri√ß√µes de coleta e amostragem, etc. Para ajudar usu√°rios da Base dos Dados terem mais contexto e entenderem melhor os dados, organize todos esses arquivos auxiliares em `/extra/auxiliary_files`.

Fique √† vontade para estruturar sub-pastas como quiser l√° dentro. O que importa √© que fique claro o que s√£o esses arquivos.

### 7. (Caso necess√°rio) Criar tabela dicion√°rio

Muitas vezes, especialmente com bases antigas, h√° m√∫ltiplos dicion√°rios
em formatos Excel ou outros. Na Base dos Dados n√≥s unificamos tudo em um
√∫nico arquivo em formato `.csv` - um √∫nico dicion√°rio para todas as
colunas de todas as tabelas do seu conjunto.

O uso de dicion√°rios tem seus pr√≥s e contras. √â bem comum, para quem trabalha
com bancos de dados relacionais (como SQLite, MySQL, MariaDB ou Postgres) o uso
de modelos relacionados para campos de categorias (como, por exemplo, na tabela da RAIS 
o campo tipo_admissao). Vejamos as vantagens e desvantagens:

- vantagens:
    - caso o usu√°rio queira uma lista de valores √∫nicos, ele pode percorrer apenas a tabela dicion√°rio, n√£o precisando percorrer toda a tabela de microdados
    - no momento do tratamento √© mais simples detectar erros de digita√ß√£o em uma determinada categoria, o que causa uma duplicidade incorreta
    - caso o usu√°rio queira exportar os dados ele obter√° um arquivo bem menor, otimizando os seus custos de transfer√™ncia
- desvantagens:
    - quando o usu√°rio precisa fazer uma consulta simples, ter√° que usar ``JOINS`` para buscar os nomes das categorias
    - ao trabalhar com tais bases, o usu√°rio precisar√° construir o modelo localmente

!!! Info "Detalhes importantes de como construir seu dicion√°rio est√£o no nosso [manual de estilo](../style_data/#dicionarios)."

#### Exemplo: RAIS - Dicion√°rio

O dicion√°rio completo [pode ser consultado
aqui](https://docs.google.com/spreadsheets/d/12Wwp48ZJVux26rCotx43lzdWmVL54JinsNnLIV3jnyM/edit?usp=sharing).
Ele j√° possui a estrutura padr√£o que utilizamos para dicion√°rios.

### 8. Subir tudo no Google Cloud

Tudo pronto! Agora s√≥ falta subir para o Google Cloud e enviar para
revis√£o. Para isso, vamos usar o cliente `basedosdados` (dispon√≠vel em
linha de comando e Python) que facilita as configura√ß√µes e etapas do processo.

#### Configure seu projeto no Google Cloud e um _bucket_ no Google Storage

Os dados v√£o passar ao todo por 3 lugares no Google Cloud:

1. **Storage**: local onde ser√£o armazenados o arquivos "frios" (arquiteturas, dados, arquivos auxiliares).
2. **BigQuery**: super banco de dados do Google, dividido em 2 projetos/tipos de tabela:
    - *Staging*: banco para teste e tratamento final do conjunto de dados
    - *Produ√ß√£o*: banco oficial de publica√ß√£o dos dados (nosso projeto `basedosdados`! ou o seu mesmo caso queira reproduzir o ambiente)

√â necess√°rio ter uma conta Google e um projeto (gratuito) no Google
Cloud. Para criar seu projeto basta:

1. Acessar o [link](https://console.cloud.google.com/projectselector2/home/dashboard) e aceitar o Termo de Servi√ßos do Google Cloud.
2. Clicar em `Create Project/Criar Projeto` - escolha um nome bacana para o seu projeto, ele ter√° tamb√©m um `Project ID` que ser√° utilizado para configura√ß√£o local.
3. Depois de criado o projeto, v√° at√© a funcionalidade de
   [Storage](https://console.cloud.google.com/storage) e crie uma pasta
   (_bucket_) para subir os dados (pode ter o mesmo nome do projeto).



#### Suba e configure uma tabela no seu _bucket_

Aqui s√£o dois passos: primeiro publicamos uma base (conjunto) e depois publicamos tabelas. Este processo
est√° automatizado na cria√ß√£o das tabelas, conforme abaixo.

<!--

Para publicar uma **base (conjunto)**:

1. Inicialize o conjunto no seu reposit√≥rio local:

    ```bash
   basedosdados dataset init [DATASET_ID]
    ```

4. Crie o conjunto no *BigQuery* rodando:

    ```bash
    basedosdados dataset create [DATASET_ID]
    ```

5. Preencha os arquivos de configura√ß√£o criados em `<dataset_id>/`:
    - `README.md`: informa√ß√µes √∫teis para quem for ver o c√≥digo no Github.
    - `dataset_config.yaml`: informa√ß√µes espec√≠ficas do conjunto (j√° vem
      com um modelo para preenchimento).
6. Atualize as informa√ß√µes preenchidas do conjunto com:

    ```bash
    basedosdados dataset update [DATASET_ID]
    ```
-->

Para publicar o dataset e a(s) **tabela(s)**:

1. Crie a tabela no *bucket*, indicando o caminho do arquivo no seu local, rodando o seguinte comando no seu terminal:

    ```bash
    basedosdados table create [DATASET_ID] [TABLE_ID] --path <caminho_para_os_dados> --force_dataset False --if_table_exists raise --if_storage_data_exists raise --if_table_config_exists raise --columns_config_url <url_da_planilha_google>
    ```

    Se preferir, voc√™ pode usar a API do Python, da seguinte forma:

    ```python
    import basedosdados as bd
    
    tb = bd.Table(dataset_id=<dataset_id>, table_id=<table_id>)
    tb.create(
        path='caminho_para_os_dados',
        force_dataset=False,
        if_table_exists='raise',
        if_storage_data_exists='raise',
        if_table_config_exists='raise',
    )
    ```

    Os seguintes par√¢metros podem ser usados (no terminal ou no Python):

    ```bash
    --path [Obrigat√≥rio] o caminho completo do arquivo no seu computador, como:
       '/Users/<seu_usuario>/projetos/basedosdados/mais/bases/[DATASET_ID]/output/microdados[.csv]'    
       caso seus dados sejam particionados, o caminho deve apontar para a pasta onde est√£o as parti√ß√µes.
       caso contr√°rio, deve apontar para um arquivo csv (por exemplo, microdados.csv)
   
    --force_dataset [True|False] cria os arquivos de configura√ß√£o do dataset locais e no BigQuery
       True: os arquivos de configura√ß√£o do dataset ser√£o criados no seu projeto e, caso ele n√£o exista
             no BigQuery, ser√° criado automaticamente. se voc√™ j√° tiver criado e configurado o dataset,
             N√ÉO USE ESTA OP√á√ÉO, POIS IR√Å SOBRESCREVER SEUS ARQUIVOS!!!!!
       False: o dataset n√£o ser√° recriado e, se n√£o existir, ser√° criado automaticamente
   
    --if_table_exists [raise|replace|pass] o que fazer se a tabela j√° existir
       raise: retorna um erro
       replace: substitui a tabela atual
       pass: n√£o faz nada

    --if_storage_data_exists [raise|replace|pass] o que fazer se os dados j√° existirem no Google Cloud Storage
       raise: retorna um erro
       replace: substitui os dados existentes
       pass: n√£o faz nada
   
    --if_table_config_exists [raise|replace|pass]
       raise: retorna mensagem de erro
       replace: substitui as configura√ß√µes da tabela existentes
       pass: n√£o faz nada
   
    --columns_config_url [google sheets url]
       a url da planilha do Google com a arquitetura. deve estar no formato 
       https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>.
       certifique-se de que a planilha est√° compartilhada com a op√ß√£o "qualquer pessoa com o link pode ver"

    ```
!!! Info "se o projeto n√£o existir no BigQuery, ele ser√° autmaticamente criado, junto com os arquivos README.md e dataset_config.yaml, que dever√£o ser preenchidos, segundo o modelo j√° criado"

2. Preencha os arquivos de configura√ß√£o da tabela:

- `/[TABLE_ID]/table_config.yaml`: informa√ß√µes espec√≠ficas da tabela.
- `/[TABLE_ID]/publish.sql`: aqui voc√™ pode indicar tratamentos finais
    na tabela `staging` em SQL para publica√ß√£o (ex: modificar a query para
    dar um JOIN em outra tabela da BD+ e selecionar vari√°veis).

3. Publique a tabela em produ√ß√£o:

    ```bash
    basedosdados table publish [DATASET_ID] [TABLE_ID]
    ```

Consulte tamb√©m nossa [API](../api_reference_cli) para mais detalhes de cada m√©todo.

!!! Tip "Abra o console do BigQuery e rode algumas _queries_ para testar se foi tudo publicado corretamente. Estamos desenvolvendo testes autom√°ticos para facilitar esse processo no futuro."

### 9. Validar os metadados para publica√ß√£o

Para validar os metadados preenchidos nos arquivos `dataset_config.yaml` e `table_config.yaml`, o processo √© simples.

<!--
1. Coloque suas credenciais do CKAN no arquivo
   `~/.basedosdados/config.toml`, preenchendo os campos `api_key` e
   `url` em `[ckan]`. Este arquivo √© criado automaticamente ao rodar
   `basedosdados config init` [(veja no passo
   7)](#7-subir-tudo-no-google-cloud).
-->
1. Valide os metadados do conjunto atrav√©s do comando `basedosdados metadata validate [DATASET_ID]`. Para validar metadados de tabelas, basta rodar `basedosdados metadata validate [DATASET_ID] [TABLE_ID]`.
2. Ap√≥s a revis√£o da nossa equipe, os dados ser√£o publicados no CKAN
<!--
3. Publique os metadados do conjunto j√° preenchidos e validados com
   `basedosdados metadata publish [DATASET_ID]`. Para publicar metadados
   de tabelas, use `basedosdados metadata publish [DATASET_ID]
   [TABLE_ID]`.

> Para publicar metadados de ambos (conjunto e tabela), use
> `basedosdados metadata publish [DATASET_ID] [TABLE_ID] --all=True`.
-->

#### Atualizando metadados de bases ou tabelas j√° existentes

Atrav√©s do m√≥dulo `metadata` √© poss√≠vel tamb√©m trabalhar com bases e tabelas j√° existentes na plataforma. Elas podem ser atualizadas a partir do procedimento que descrevemos a seguir.

1. Rode `basedosdados metadata create [DATASET_ID]` para puxar os
   metadados do conjunto dispon√≠veis no site para o arquivo
   `dataset_config.yaml`. Para puxar metadados de tabelas, use
   `basedosdados metadata create [DATASET_ID] [TABLE_ID]`, que ir√°
   atualizo arquivo `table_config.yaml`
2. Preencha os novos valores de metadados nos respectivos arquivos.
3. Rode `basedosdados metadata validate [DATASET_ID]` <!-- e para publicar os
   metadados atualizados do conjutno use `basedosdados metadata publish [DATASET_ID]`. O mesmo pode ser feito
   para tabela adicionando o parametro `[TABLE_ID]` -->

### 10. Enviar tudo para revis√£o

Ufa, √© isso! Agora s√≥ resta enviar tudo para revis√£o no
[reposit√≥rio](https://github.com/basedosdados/mais) da Base dos Dados.
Crie os _commits_ necess√°rios e rode `git push origin [BRANCH_ID]`.
Depois √© s√≥ abrir um _pull request_ (PR) no nosso reposit√≥rio.

**E agora?** Nossa equipe ir√° revisar os dados e metadados submetidos
via Github. Podemos entrar em contato para tirar d√∫vidas ou solicitar
mudan√ßas no c√≥digo. Quando tudo estiver OK, fazemos um _merge_ do seu
*pull request*, e os dados s√£o automaticamente publicados na nossa
plataforma!
