{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[dados] br_mc_auxilio_emergencial",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEeLrzrYVbDL",
        "outputId": "92f62b88-b79d-4dae-e6e3-ba4d2ac0cd71"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFw6MkVKXXCo"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import zipfile\n",
        "from zipfile import ZipFile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euMtLfszLJPL"
      },
      "source": [
        "#criação do diretorio temporário pra salvar csv particionado antes do tratamento \n",
        "ano = ['2020-04', '2020-05', '2020-06', '2020-07', '2020-08', '2020-09', '2020-10',\n",
        "       '2020-11', '2020-12', '2021-01', '2021-02', '2021-03', '2021-04', '2021-05',\n",
        "       '2021-06', '2021-07']\n",
        "\n",
        "ufs = [\"AC\", \"AL\", \"AM\", \"AP\", \"BA\", \"CE\", \"DF\", \"ES\", \"GO\",\n",
        "      \"MA\", \"MG\", \"MS\", \"MT\", \"PA\", \"PB\", \"PE\", \"PI\", \"PR\",\n",
        "      \"RJ\", \"RN\", \"RO\", \"RR\", \"SC\", \"SE\", \"RS\", \"SP\", \"TO\"]\n",
        "\n",
        "#for i in ano:\n",
        "#    for uf in ufs:\n",
        "#        os.makedirs('/content/gdrive/MyDrive/br_mc_auxilio_emergencial/output/mes={}/sigla_uf={}'.format(i, uf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlU-YvX_0kNx"
      },
      "source": [
        "rename = {'MÊS DISPONIBILIZAÇÃO':'mes', 'UF':'sigla_uf', 'CÓDIGO MUNICÍPIO IBGE':'id_municipio', 'NIS BENEFICIÁRIO':'nis_beneficiario', \n",
        "          'CPF BENEFICIÁRIO':'cpf_beneficiario', 'NOME BENEFICIÁRIO':'nome_beneficiario', 'NIS RESPONSÁVEL':'nis_responsavel',\n",
        "          'CPF RESPONSÁVEL':'cpf_responsavel', 'NOME RESPONSÁVEL':'nome_responsavel', 'ENQUADRAMENTO':'enquadramento', \n",
        "          'PARCELA':'parcela', 'OBSERVAÇÃO':'observacao', 'VALOR BENEFÍCIO':'valor_beneficio'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vft4bHih0k6j"
      },
      "source": [
        "ordem = ['mes', 'sigla_uf', 'id_municipio', 'nis_beneficiario', 'cpf_beneficiario', 'nome_beneficiario', 'nis_responsavel', 'cpf_responsavel', \n",
        "         'nome_responsavel', 'enquadramento', 'parcela', 'observacao', 'valor_beneficio']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHYiCqCZS4kJ"
      },
      "source": [
        "##### **Auxílio Emergencial 2020**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSDfWrWy5yKk"
      },
      "source": [
        "abril = '/content/gdrive/MyDrive/br_mc_auxilio_emergencial/input/202004_AuxilioEmergencial.zip'\n",
        "extract = '/content/gdrive/MyDrive/br_mc_auxilio_emergencial/temp/mes=2020-04'\n",
        "with zipfile.ZipFile(abril, 'r') as zip_ref:\n",
        "  zip_ref.extractall(extract)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Irx53jUL5pU"
      },
      "source": [
        "for uf in ufs:\n",
        "      auxilio_abril = pd.DataFrame()\n",
        "      ch=0\n",
        "      for chunk in pd.read_csv(extract+'/202004_AuxilioEmergencial.csv',sep=';', encoding='latin-1', dtype='string', chunksize=500000):\n",
        "        ch = ch + 1\n",
        "        print(\"Fazendo chunk parte {}\".format(ch))\n",
        "        chunk.rename(columns=rename, inplace=True)\n",
        "        chunk = chunk[chunk['sigla_uf']==uf]\n",
        "        chunk['mes'] =  pd.to_datetime(chunk['mes'], format='%Y%m', errors='coerce').dt.to_period('m')\n",
        "        chunk['valor_beneficio'] = chunk['valor_beneficio'].apply(lambda x: str(x).replace(',','.'))\n",
        "        chunk.loc[(chunk['nis_beneficiario'] == '00000000000'), 'nis_beneficiario'] = ''\n",
        "        chunk.loc[(chunk['nis_responsavel'] == '-2'), 'nis_responsavel'] = ''\n",
        "        chunk['cpf_responsavel'] = chunk['cpf_responsavel'].fillna('')\n",
        "        chunk.loc[(chunk['nome_responsavel'] == 'Não se aplica'), 'nome_responsavel'] = ''\n",
        "        chunk.loc[(chunk['observacao'] == 'Não há'), 'observacao'] = ''\n",
        "        chunk['observacao'] = chunk['observacao'].fillna('')\n",
        "        chunk.sort_values(['nis_beneficiario', 'nis_responsavel', 'cpf_beneficiario', 'cpf_responsavel'], inplace=True)\n",
        "        chunk = chunk[ordem]\n",
        "        chunk.drop(['mes', 'sigla_uf'], axis=1, inplace=True)\n",
        "        auxilio_abril = auxilio_abril.append(chunk)\n",
        "      auxilio_abril.drop_duplicates(subset=['nis_beneficiario', 'nis_responsavel', 'cpf_beneficiario', 'cpf_responsavel', 'nome_beneficiario', 'observacao'], keep ='first', inplace = True)\n",
        "      exec('auxilio_abril.to_csv(\"/content/gdrive/MyDrive/br_mc_auxilio_emergencial/output/mes=2020-04/sigla_uf={}/microdados.csv\",index=False, encoding=\"utf-8\", na_rep=\"\")'.format(uf))\n",
        "del auxilio_abril"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSbo-ry5Pk-9"
      },
      "source": [
        "maio = '/content/gdrive/MyDrive/br_mc_auxilio_emergencial/input/202005_AuxilioEmergencial.zip'\n",
        "extract = '/content/gdrive/MyDrive/br_mc_auxilio_emergencial/temp/mes=2020-05'\n",
        "with zipfile.ZipFile(maio, 'r') as zip_ref:\n",
        "  zip_ref.extractall(extract)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcHjN03QPrfY"
      },
      "source": [
        "for uf in ufs:\n",
        "      auxilio_maio = pd.DataFrame()\n",
        "      ch=0\n",
        "      for chunk in pd.read_csv(extract+'/202005_AuxilioEmergencial.csv',sep=';', encoding='latin-1', dtype='string', chunksize=500000):\n",
        "        ch = ch + 1\n",
        "        print(\"Fazendo chunk parte {}\".format(ch))\n",
        "        chunk.rename(columns=rename, inplace=True)\n",
        "        chunk = chunk[chunk['sigla_uf']==uf]\n",
        "        chunk['mes'] =  pd.to_datetime(chunk['mes'], format='%Y%m', errors='coerce').dt.to_period('m')\n",
        "        chunk['valor_beneficio'] = chunk['valor_beneficio'].apply(lambda x: str(x).replace(',','.'))\n",
        "        chunk.loc[(chunk['nis_beneficiario'] == '00000000000'), 'nis_beneficiario'] = ''\n",
        "        chunk.loc[(chunk['nis_responsavel'] == '-2'), 'nis_responsavel'] = ''\n",
        "        chunk['cpf_responsavel'] = chunk['cpf_responsavel'].fillna('')\n",
        "        chunk.loc[(chunk['nome_responsavel'] == 'Não se aplica'), 'nome_responsavel'] = ''\n",
        "        chunk.loc[(chunk['observacao'] == 'Não há'), 'observacao'] = ''\n",
        "        chunk['observacao'] = chunk['observacao'].fillna('')\n",
        "        chunk.sort_values(['nis_beneficiario', 'nis_responsavel', 'cpf_beneficiario', 'cpf_responsavel'], inplace=True)\n",
        "        chunk = chunk[ordem]\n",
        "        chunk.drop(['mes', 'sigla_uf'], axis=1, inplace=True)\n",
        "        auxilio_maio = auxilio_maio.append(chunk)\n",
        "      auxilio_maio.drop_duplicates(subset=['nis_beneficiario', 'nis_responsavel', 'cpf_beneficiario', 'cpf_responsavel', 'nome_beneficiario', 'observacao'], keep ='first', inplace = True)\n",
        "      exec('auxilio_maio.to_csv(\"/content/gdrive/MyDrive/br_mc_auxilio_emergencial/output/mes=2020-05/sigla_uf={}/microdados.csv\",index=False, encoding=\"utf-8\", na_rep=\"\")'.format(uf))\n",
        "del auxilio_maio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi2jDlfgLj3Q"
      },
      "source": [
        "junho = '/content/gdrive/MyDrive/br_mc_auxilio_emergencial/input/202006_AuxilioEmergencial.zip'\n",
        "extract = '/content/gdrive/MyDrive/br_mc_auxilio_emergencial/temp/mes=2020-06'\n",
        "with zipfile.ZipFile(junho, 'r') as zip_ref:\n",
        "  zip_ref.extractall(extract)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW8CK99kPHd1"
      },
      "source": [
        "for uf in ufs:\n",
        "      auxilio_junho = pd.DataFrame()\n",
        "      ch=0\n",
        "      for chunk in pd.read_csv(extract+'/202006_AuxilioEmergencial.csv',sep=';', encoding='latin-1', dtype='string', chunksize=500000):\n",
        "        ch = ch + 1\n",
        "        print(\"Fazendo chunk parte {}\".format(ch))\n",
        "        chunk.rename(columns=rename, inplace=True)\n",
        "        chunk = chunk[chunk['sigla_uf']==uf]\n",
        "        chunk['mes'] =  pd.to_datetime(chunk['mes'], format='%Y%m', errors='coerce').dt.to_period('m')\n",
        "        chunk['valor_beneficio'] = chunk['valor_beneficio'].apply(lambda x: str(x).replace(',','.'))\n",
        "        chunk.loc[(chunk['nis_beneficiario'] == '00000000000'), 'nis_beneficiario'] = ''\n",
        "        chunk.loc[(chunk['nis_responsavel'] == '-2'), 'nis_responsavel'] = ''\n",
        "        chunk['cpf_responsavel'] = chunk['cpf_responsavel'].fillna('')\n",
        "        chunk.loc[(chunk['nome_responsavel'] == 'Não se aplica'), 'nome_responsavel'] = ''\n",
        "        chunk.loc[(chunk['observacao'] == 'Não há'), 'observacao'] = ''\n",
        "        chunk['observacao'] = chunk['observacao'].fillna('')\n",
        "        chunk.sort_values(['nis_beneficiario', 'nis_responsavel', 'cpf_beneficiario', 'cpf_responsavel'], inplace=True)\n",
        "        chunk = chunk[ordem]\n",
        "        chunk.drop(['mes', 'sigla_uf'], axis=1, inplace=True)\n",
        "        auxilio_junho = auxilio_junho.append(chunk)\n",
        "      auxilio_junho.drop_duplicates(subset=['nis_beneficiario', 'nis_responsavel', 'cpf_beneficiario', 'cpf_responsavel', 'nome_beneficiario', 'observacao'], keep ='first', inplace = True)\n",
        "      exec('auxilio_junho.to_csv(\"/content/gdrive/MyDrive/br_mc_auxilio_emergencial/output/mes=2020-06/sigla_uf={}/microdados.csv\",index=False, encoding=\"utf-8\", na_rep=\"\")'.format(uf))\n",
        "del auxilio_junho"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvpRRfRTODUJ"
      },
      "source": [
        "julho = '/content/gdrive/MyDrive/br_mc_auxilio_emergencial/input/202007_AuxilioEmergencial.zip'\n",
        "extract = '/content/gdrive/MyDrive/br_mc_auxilio_emergencial/temp/mes=2020-07'\n",
        "with zipfile.ZipFile(julho, 'r') as zip_ref:\n",
        "  zip_ref.extractall(extract)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7MvFGYRQHz1"
      },
      "source": [
        "for uf in ufs:\n",
        "      auxilio_julho = pd.DataFrame()\n",
        "      ch=0\n",
        "      for chunk in pd.read_csv(extract+'/202007_AuxilioEmergencial.csv',sep=';', encoding='latin-1', dtype='string', chunksize=500000):\n",
        "        ch = ch + 1\n",
        "        print(\"Fazendo chunk parte {}\".format(ch))\n",
        "        chunk.rename(columns=rename, inplace=True)\n",
        "        chunk = chunk[chunk['sigla_uf']==uf]\n",
        "        chunk['mes'] =  pd.to_datetime(chunk['mes'], format='%Y%m', errors='coerce').dt.to_period('m')\n",
        "        chunk['valor_beneficio'] = chunk['valor_beneficio'].apply(lambda x: str(x).replace(',','.'))\n",
        "        chunk.loc[(chunk['nis_beneficiario'] == '00000000000'), 'nis_beneficiario'] = ''\n",
        "        chunk.loc[(chunk['nis_responsavel'] == '-2'), 'nis_responsavel'] = ''\n",
        "        chunk['cpf_responsavel'] = chunk['cpf_responsavel'].fillna('')\n",
        "        chunk.loc[(chunk['nome_responsavel'] == 'Não se aplica'), 'nome_responsavel'] = ''\n",
        "        chunk.loc[(chunk['observacao'] == 'Não há'), 'observacao'] = ''\n",
        "        chunk['observacao'] = chunk['observacao'].fillna('')\n",
        "        chunk.sort_values(['nis_beneficiario', 'nis_responsavel', 'cpf_beneficiario', 'cpf_responsavel'], inplace=True)\n",
        "        chunk = chunk[ordem]\n",
        "        chunk.drop(['mes', 'sigla_uf'], axis=1, inplace=True)\n",
        "        auxilio_julho = auxilio_julho.append(chunk)\n",
        "      auxilio_julho.drop_duplicates(subset=['nis_beneficiario', 'nis_responsavel', 'cpf_beneficiario', 'cpf_responsavel', 'nome_beneficiario', 'observacao'], keep ='first', inplace = True)\n",
        "      exec('auxilio_julho.to_csv(\"/content/gdrive/MyDrive/br_mc_auxilio_emergencial/output/mes=2020-07/sigla_uf={}/microdados.csv\",index=False, encoding=\"utf-8\", na_rep=\"\")'.format(uf))\n",
        "del auxilio_julho"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IffVNgLOOPFd"
      },
      "source": [
        "agosto = '/content/gdrive/MyDrive/br_mc_auxilio_emergencial/input/202008_AuxilioEmergencial.zip'\n",
        "extract = '/content/gdrive/MyDrive/br_mc_auxilio_emergencial/temp/mes=2020-08'\n",
        "with zipfile.ZipFile(agosto, 'r') as zip_ref:\n",
        "  zip_ref.extractall(extract)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHj8gtRmQV3B"
      },
      "source": [
        "for uf in ufs:\n",
        "      auxilio_agosto = pd.DataFrame()\n",
        "      ch=0\n",
        "      for chunk in pd.read_csv(extract+'/202008_AuxilioEmergencial.csv',sep=';', encoding='latin-1', dtype='string', chunksize=500000):\n",
        "        ch = ch + 1\n",
        "        print(\"Fazendo chunk parte {}\".format(ch))\n",
        "        chunk.rename(columns=rename, inplace=True)\n",
        "        chunk = chunk[chunk['sigla_uf']==uf]\n",
        "        chunk['mes'] =  pd.to_datetime(chunk['mes'], format='%Y%m', errors='coerce').dt.to_period('m')\n",
        "        chunk['valor_beneficio'] = chunk['valor_beneficio'].apply(lambda x: str(x).replace(',','.'))\n",
        "        chunk.loc[(chunk['nis_beneficiario'] == '00000000000'), 'nis_beneficiario'] = ''\n",
        "        chunk.loc[(chunk['nis_responsavel'] == '-2'), 'nis_responsavel'] = ''\n",
        "        chunk['cpf_responsavel'] = chunk['cpf_responsavel'].fillna('')\n",
        "        chunk.loc[(chunk['nome_responsavel'] == 'Não se aplica'), 'nome_responsavel'] = ''\n",
        "        chunk.loc[(chunk['observacao'] == 'Não há'), 'observacao'] = ''\n",
        "        chunk['observacao'] = chunk['observacao'].fillna('')\n",
        "        chunk.sort_values(['nis_beneficiario', 'nis_responsavel', 'cpf_beneficiario', 'cpf_responsavel'], inplace=True)\n",
        "        chunk = chunk[ordem]\n",
        "        chunk.drop(['mes', 'sigla_uf'], axis=1, inplace=True)\n",
        "        auxilio_agosto = auxilio_agosto.append(chunk)\n",
        "      auxilio_agosto.drop_duplicates(subset=['nis_beneficiario', 'nis_responsavel', 'cpf_beneficiario', 'cpf_responsavel', 'nome_beneficiario', 'observacao'], keep ='first', inplace = True)\n",
        "      exec('auxilio_agosto.to_csv(\"/content/gdrive/MyDrive/br_mc_auxilio_emergencial/output/mes=2020-08/sigla_uf={}/microdados.csv\",index=False, encoding=\"utf-8\", na_rep=\"\")'.format(uf))\n",
        "del auxilio_agosto"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXe69PwGOTWD"
      },
      "source": [
        "setembro = '/content/gdrive/MyDrive/br_mc_auxilio_emergencial/input/202009_AuxilioEmergencial.zip'\n",
        "extract = '/content/gdrive/MyDrive/br_mc_auxilio_emergencial/temp/mes=2020-09'\n",
        "with zipfile.ZipFile(setembro, 'r') as zip_ref:\n",
        "  zip_ref.extractall(extract)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rAZyzc9QsGX"
      },
      "source": [
        "for uf in ufs:\n",
        "      auxilio_setembro = pd.DataFrame()\n",
        "      ch=0\n",
        "      for chunk in pd.read_csv(extract+'/202009_AuxilioEmergencial.csv',sep=';', encoding='latin-1', dtype='string', chunksize=500000):\n",
        "        ch = ch + 1\n",
        "        print(\"Fazendo chunk parte {}\".format(ch))\n",
        "        chunk.rename(columns=rename, inplace=True)\n",
        "        chunk = chunk[chunk['sigla_uf']==uf]\n",
        "        chunk['mes'] =  pd.to_datetime(chunk['mes'], format='%Y%m', errors='coerce').dt.to_period('m')\n",
        "        chunk['valor_beneficio'] = chunk['valor_beneficio'].apply(lambda x: str(x).replace(',','.'))\n",
        "        chunk.loc[(chunk['nis_beneficiario'] == '00000000000'), 'nis_beneficiario'] = ''\n",
        "        chunk.loc[(chunk['nis_responsavel'] == '-2'), 'nis_responsavel'] = ''\n",
        "        chunk['cpf_responsavel'] = chunk['cpf_responsavel'].fillna('')\n",
        "        chunk.loc[(chunk['nome_responsavel'] == 'Não se aplica'), 'nome_responsavel'] = ''\n",
        "        chunk.loc[(chunk['observacao'] == 'Não há'), 'observacao'] = ''\n",
        "        chunk['observacao'] = chunk['observacao'].fillna('')\n",
        "        chunk.sort_values(['nis_beneficiario', 'nis_responsavel', 'cpf_beneficiario', 'cpf_responsavel'], inplace=True)\n",
        "        chunk = chunk[ordem]\n",
        "        chunk.drop(['mes', 'sigla_uf'], axis=1, inplace=True)\n",
        "        auxilio_setembro = auxilio_setembro.append(chunk)\n",
        "      auxilio_setembro.drop_duplicates(subset=['nis_beneficiario', 'nis_responsavel', 'cpf_beneficiario', 'cpf_responsavel', 'nome_beneficiario', 'observacao'], keep ='first', inplace = True)\n",
        "      exec('auxilio_setembro.to_csv(\"/content/gdrive/MyDrive/br_mc_auxilio_emergencial/output/mes=2020-09/sigla_uf={}/microdados.csv\",index=False, encoding=\"utf-8\", na_rep=\"\")'.format(uf))\n",
        "del auxilio_setembro"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88zHa-B3OYVF"
      },
      "source": [
        "outubro = '/content/gdrive/MyDrive/br_mc_auxilio_emergencial/input/202010_AuxilioEmergencial.zip'\n",
        "extract = '/content/gdrive/MyDrive/br_mc_auxilio_emergencial/temp/mes=2020-10'\n",
        "with zipfile.ZipFile(outubro, 'r') as zip_ref:\n",
        "  zip_ref.extractall(extract)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv80B4SpRCOU"
      },
      "source": [
        "for uf in ufs:\n",
        "      auxilio_outubro = pd.DataFrame()\n",
        "      ch=0\n",
        "      for chunk in pd.read_csv(extract+'/202010_AuxilioEmergencial.csv',sep=';', encoding='latin-1', dtype='string', chunksize=500000):\n",
        "        ch = ch + 1\n",
        "        print(\"Fazendo chunk parte {}\".format(ch))\n",
        "        chunk.rename(columns=rename, inplace=True)\n",
        "        chunk = chunk[chunk['sigla_uf']==uf]\n",
        "        chunk['mes'] =  pd.to_datetime(chunk['mes'], format='%Y%m', errors='coerce').dt.to_period('m')\n",
        "        chunk['valor_beneficio'] = chunk['valor_beneficio'].apply(lambda x: str(x).replace(',','.'))\n",
        "        chunk.loc[(chunk['nis_beneficiario'] == '00000000000'), 'nis_beneficiario'] = ''\n",
        "        chunk.loc[(chunk['nis_responsavel'] == '-2'), 'nis_responsavel'] = ''\n",
        "        chunk['cpf_responsavel'] = chunk['cpf_responsavel'].fillna('')\n",
        "        chunk.loc[(chunk['nome_responsavel'] == 'Não se aplica'), 'nome_responsavel'] = ''\n",
        "        chunk.loc[(chunk['observacao'] == 'Não há'), 'observacao'] = ''\n",
        "        chunk['observacao'] = chunk['observacao'].fillna('')\n",
        "        chunk.sort_values(['nis_beneficiario', 'nis_responsavel', 'cpf_beneficiario', 'cpf_responsavel'], inplace=True)\n",
        "        chunk = chunk[ordem]\n",
        "        chunk.drop(['mes', 'sigla_uf'], axis=1, inplace=True)\n",
        "        auxilio_outubro = auxilio_outubro.append(chunk)\n",
        "      auxilio_outubro.drop_duplicates(subset=['nis_beneficiario', 'nis_responsavel', 'cpf_beneficiario', 'cpf_responsavel', 'nome_beneficiario', 'observacao'], keep ='first', inplace = True)\n",
        "      exec('auxilio_outubro.to_csv(\"/content/gdrive/MyDrive/br_mc_auxilio_emergencial/output/mes=2020-10/sigla_uf={}/microdados.csv\",index=False, encoding=\"utf-8\", na_rep=\"\")'.format(uf))\n",
        "del auxilio_outubro"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZBmkfklOcWm"
      },
      "source": [
        "novembro = '/content/gdrive/MyDrive/br_mc_auxilio_emergencial/input/202011_AuxilioEmergencial.zip'\n",
        "extract = '/content/gdrive/MyDrive/br_mc_auxilio_emergencial/temp/mes=2020-11'\n",
        "with zipfile.ZipFile(novembro, 'r') as zip_ref:\n",
        "  zip_ref.extractall(extract)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8SPNxN5RCwV"
      },
      "source": [
        "for uf in ufs:\n",
        "      auxilio_novembro = pd.DataFrame()\n",
        "      ch=0\n",
        "      for chunk in pd.read_csv(extract+'/202011_AuxilioEmergencial.csv',sep=';', encoding='latin-1', dtype='string', chunksize=500000):\n",
        "        ch = ch + 1\n",
        "        print(\"Fazendo chunk parte {}\".format(ch))\n",
        "        chunk.rename(columns=rename, inplace=True)\n",
        "        chunk = chunk[chunk['sigla_uf']==uf]\n",
        "        chunk['mes'] =  pd.to_datetime(chunk['mes'], format='%Y%m', errors='coerce').dt.to_period('m')\n",
        "        chunk['valor_beneficio'] = chunk['valor_beneficio'].apply(lambda x: str(x).replace(',','.'))\n",
        "        chunk.loc[(chunk['nis_beneficiario'] == '00000000000'), 'nis_beneficiario'] = ''\n",
        "        chunk.loc[(chunk['nis_responsavel'] == '-2'), 'nis_responsavel'] = ''\n",
        "        chunk['cpf_responsavel'] = chunk['cpf_responsavel'].fillna('')\n",
        "        chunk.loc[(chunk['nome_responsavel'] == 'Não se aplica'), 'nome_responsavel'] = ''\n",
        "        chunk.loc[(chunk['observacao'] == 'Não há'), 'observacao'] = ''\n",
        "        chunk['observacao'] = chunk['observacao'].fillna('')\n",
        "        chunk.sort_values(['nis_beneficiario', 'nis_responsavel', 'cpf_beneficiario', 'cpf_responsavel'], inplace=True)\n",
        "        chunk = chunk[ordem]\n",
        "        chunk.drop(['mes', 'sigla_uf'], axis=1, inplace=True)\n",
        "        auxilio_novembro = auxilio_novembro.append(chunk)\n",
        "      auxilio_novembro.drop_duplicates(subset=['nis_beneficiario', 'nis_responsavel', 'cpf_beneficiario', 'cpf_responsavel', 'nome_beneficiario', 'observacao'], keep ='first', inplace = True)\n",
        "      exec('auxilio_novembro.to_csv(\"/content/gdrive/MyDrive/br_mc_auxilio_emergencial/output/mes=2020-11/sigla_uf={}/microdados.csv\",index=False, encoding=\"utf-8\", na_rep=\"\")'.format(uf))\n",
        "del auxilio_novembro"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJKYr3vcOiLY"
      },
      "source": [
        "dezembro = '/content/gdrive/MyDrive/br_mc_auxilio_emergencial/input/202012_AuxilioEmergencial.zip'\n",
        "extract = '/content/gdrive/MyDrive/br_mc_auxilio_emergencial/temp/mes=2020-12'\n",
        "with zipfile.ZipFile(dezembro, 'r') as zip_ref:\n",
        "  zip_ref.extractall(extract)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp4xGwmxPqeA"
      },
      "source": [
        "ufs = {'SP'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1expD1asRDVv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d15c12e8-a588-4d05-8f6d-3d979dee2f69"
      },
      "source": [
        "for uf in ufs:\n",
        "      auxilio_dezembro = pd.DataFrame()\n",
        "      ch=0\n",
        "      for chunk in pd.read_csv(extract+'/202012_AuxilioEmergencial.csv',sep=';', encoding='latin-1', dtype='string', chunksize=500000):\n",
        "        ch = ch + 1\n",
        "        print(\"Fazendo chunk parte {}\".format(ch))\n",
        "        chunk.rename(columns=rename, inplace=True)\n",
        "        chunk = chunk[chunk['sigla_uf']==uf]\n",
        "        chunk['mes'] =  pd.to_datetime(chunk['mes'], format='%Y%m', errors='coerce').dt.to_period('m')\n",
        "        chunk['valor_beneficio'] = chunk['valor_beneficio'].apply(lambda x: str(x).replace(',','.'))\n",
        "        chunk.loc[(chunk['nis_beneficiario'] == '00000000000'), 'nis_beneficiario'] = ''\n",
        "        chunk.loc[(chunk['nis_responsavel'] == '-2'), 'nis_responsavel'] = ''\n",
        "        chunk['cpf_responsavel'] = chunk['cpf_responsavel'].fillna('')\n",
        "        chunk.loc[(chunk['nome_responsavel'] == 'Não se aplica'), 'nome_responsavel'] = ''\n",
        "        chunk.loc[(chunk['observacao'] == 'Não há'), 'observacao'] = ''\n",
        "        chunk['observacao'] = chunk['observacao'].fillna('')\n",
        "        chunk.sort_values(['nis_beneficiario', 'nis_responsavel', 'cpf_beneficiario', 'cpf_responsavel'], inplace=True)\n",
        "        chunk = chunk[ordem]\n",
        "        chunk.drop(['mes', 'sigla_uf'], axis=1, inplace=True)\n",
        "        auxilio_dezembro = auxilio_dezembro.append(chunk)\n",
        "      auxilio_dezembro.drop_duplicates(subset=['nis_beneficiario', 'nis_responsavel', 'cpf_beneficiario', 'cpf_responsavel', 'nome_beneficiario', 'observacao'], keep ='first', inplace = True)\n",
        "      exec('auxilio_dezembro.to_csv(\"/content/gdrive/MyDrive/br_mc_auxilio_emergencial/output/mes=2020-12/sigla_uf={}/microdados.csv\",index=False, encoding=\"utf-8\", na_rep=\"\")'.format(uf))\n",
        "del auxilio_dezembro"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fazendo chunk parte 1\n",
            "Fazendo chunk parte 2\n",
            "Fazendo chunk parte 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fazendo chunk parte 4\n",
            "Fazendo chunk parte 5\n",
            "Fazendo chunk parte 6\n",
            "Fazendo chunk parte 7\n",
            "Fazendo chunk parte 8\n",
            "Fazendo chunk parte 9\n",
            "Fazendo chunk parte 10\n",
            "Fazendo chunk parte 11\n",
            "Fazendo chunk parte 12\n",
            "Fazendo chunk parte 13\n",
            "Fazendo chunk parte 14\n",
            "Fazendo chunk parte 15\n",
            "Fazendo chunk parte 16\n",
            "Fazendo chunk parte 17\n",
            "Fazendo chunk parte 18\n",
            "Fazendo chunk parte 19\n",
            "Fazendo chunk parte 20\n",
            "Fazendo chunk parte 21\n",
            "Fazendo chunk parte 22\n",
            "Fazendo chunk parte 23\n",
            "Fazendo chunk parte 24\n",
            "Fazendo chunk parte 25\n",
            "Fazendo chunk parte 26\n",
            "Fazendo chunk parte 27\n",
            "Fazendo chunk parte 28\n",
            "Fazendo chunk parte 29\n",
            "Fazendo chunk parte 30\n",
            "Fazendo chunk parte 31\n",
            "Fazendo chunk parte 32\n",
            "Fazendo chunk parte 33\n",
            "Fazendo chunk parte 34\n",
            "Fazendo chunk parte 35\n",
            "Fazendo chunk parte 36\n",
            "Fazendo chunk parte 37\n",
            "Fazendo chunk parte 38\n",
            "Fazendo chunk parte 39\n",
            "Fazendo chunk parte 40\n",
            "Fazendo chunk parte 41\n",
            "Fazendo chunk parte 42\n",
            "Fazendo chunk parte 43\n",
            "Fazendo chunk parte 44\n",
            "Fazendo chunk parte 45\n",
            "Fazendo chunk parte 46\n",
            "Fazendo chunk parte 47\n",
            "Fazendo chunk parte 48\n",
            "Fazendo chunk parte 49\n",
            "Fazendo chunk parte 50\n",
            "Fazendo chunk parte 51\n",
            "Fazendo chunk parte 52\n",
            "Fazendo chunk parte 53\n",
            "Fazendo chunk parte 54\n",
            "Fazendo chunk parte 55\n",
            "Fazendo chunk parte 56\n",
            "Fazendo chunk parte 57\n",
            "Fazendo chunk parte 58\n",
            "Fazendo chunk parte 59\n",
            "Fazendo chunk parte 60\n",
            "Fazendo chunk parte 61\n",
            "Fazendo chunk parte 62\n",
            "Fazendo chunk parte 63\n",
            "Fazendo chunk parte 64\n",
            "Fazendo chunk parte 65\n",
            "Fazendo chunk parte 66\n",
            "Fazendo chunk parte 67\n",
            "Fazendo chunk parte 68\n",
            "Fazendo chunk parte 69\n",
            "Fazendo chunk parte 70\n",
            "Fazendo chunk parte 71\n",
            "Fazendo chunk parte 72\n",
            "Fazendo chunk parte 73\n",
            "Fazendo chunk parte 74\n",
            "Fazendo chunk parte 75\n",
            "Fazendo chunk parte 76\n",
            "Fazendo chunk parte 77\n",
            "Fazendo chunk parte 78\n",
            "Fazendo chunk parte 79\n",
            "Fazendo chunk parte 80\n",
            "Fazendo chunk parte 81\n",
            "Fazendo chunk parte 82\n",
            "Fazendo chunk parte 83\n",
            "Fazendo chunk parte 84\n",
            "Fazendo chunk parte 85\n",
            "Fazendo chunk parte 86\n",
            "Fazendo chunk parte 87\n",
            "Fazendo chunk parte 88\n",
            "Fazendo chunk parte 89\n",
            "Fazendo chunk parte 90\n",
            "Fazendo chunk parte 91\n",
            "Fazendo chunk parte 92\n",
            "Fazendo chunk parte 93\n",
            "Fazendo chunk parte 94\n",
            "Fazendo chunk parte 95\n",
            "Fazendo chunk parte 96\n",
            "Fazendo chunk parte 97\n",
            "Fazendo chunk parte 98\n",
            "Fazendo chunk parte 99\n",
            "Fazendo chunk parte 100\n",
            "Fazendo chunk parte 101\n",
            "Fazendo chunk parte 102\n",
            "Fazendo chunk parte 103\n",
            "Fazendo chunk parte 104\n",
            "Fazendo chunk parte 105\n",
            "Fazendo chunk parte 106\n",
            "Fazendo chunk parte 107\n",
            "Fazendo chunk parte 108\n",
            "Fazendo chunk parte 109\n",
            "Fazendo chunk parte 110\n",
            "Fazendo chunk parte 111\n",
            "Fazendo chunk parte 112\n",
            "Fazendo chunk parte 113\n",
            "Fazendo chunk parte 114\n",
            "Fazendo chunk parte 115\n",
            "Fazendo chunk parte 116\n",
            "Fazendo chunk parte 117\n",
            "Fazendo chunk parte 118\n",
            "Fazendo chunk parte 119\n",
            "Fazendo chunk parte 120\n",
            "Fazendo chunk parte 121\n",
            "Fazendo chunk parte 122\n",
            "Fazendo chunk parte 123\n",
            "Fazendo chunk parte 124\n",
            "Fazendo chunk parte 125\n",
            "Fazendo chunk parte 126\n",
            "Fazendo chunk parte 127\n",
            "Fazendo chunk parte 128\n",
            "Fazendo chunk parte 129\n",
            "Fazendo chunk parte 130\n",
            "Fazendo chunk parte 131\n",
            "Fazendo chunk parte 132\n",
            "Fazendo chunk parte 133\n",
            "Fazendo chunk parte 134\n",
            "Fazendo chunk parte 135\n",
            "Fazendo chunk parte 136\n",
            "Fazendo chunk parte 137\n",
            "Fazendo chunk parte 138\n",
            "Fazendo chunk parte 139\n",
            "Fazendo chunk parte 140\n",
            "Fazendo chunk parte 141\n",
            "Fazendo chunk parte 142\n",
            "Fazendo chunk parte 143\n",
            "Fazendo chunk parte 144\n",
            "Fazendo chunk parte 145\n",
            "Fazendo chunk parte 146\n",
            "Fazendo chunk parte 147\n",
            "Fazendo chunk parte 148\n",
            "Fazendo chunk parte 149\n",
            "Fazendo chunk parte 150\n",
            "Fazendo chunk parte 151\n",
            "Fazendo chunk parte 152\n",
            "Fazendo chunk parte 153\n",
            "Fazendo chunk parte 154\n",
            "Fazendo chunk parte 155\n",
            "Fazendo chunk parte 156\n",
            "Fazendo chunk parte 157\n",
            "Fazendo chunk parte 158\n",
            "Fazendo chunk parte 159\n",
            "Fazendo chunk parte 160\n",
            "Fazendo chunk parte 161\n",
            "Fazendo chunk parte 162\n",
            "Fazendo chunk parte 163\n",
            "Fazendo chunk parte 164\n",
            "Fazendo chunk parte 165\n",
            "Fazendo chunk parte 166\n",
            "Fazendo chunk parte 167\n",
            "Fazendo chunk parte 168\n",
            "Fazendo chunk parte 169\n",
            "Fazendo chunk parte 170\n",
            "Fazendo chunk parte 171\n",
            "Fazendo chunk parte 172\n",
            "Fazendo chunk parte 173\n",
            "Fazendo chunk parte 174\n",
            "Fazendo chunk parte 175\n",
            "Fazendo chunk parte 176\n",
            "Fazendo chunk parte 177\n",
            "Fazendo chunk parte 178\n",
            "Fazendo chunk parte 179\n",
            "Fazendo chunk parte 180\n",
            "Fazendo chunk parte 181\n",
            "Fazendo chunk parte 182\n",
            "Fazendo chunk parte 183\n",
            "Fazendo chunk parte 184\n",
            "Fazendo chunk parte 185\n",
            "Fazendo chunk parte 186\n",
            "Fazendo chunk parte 187\n",
            "Fazendo chunk parte 188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kxC51bzSkGQ"
      },
      "source": [
        "##### **Auxílio Emergencial 2021**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdLUIGlCSjuJ"
      },
      "source": [
        "meses = ['2021-01', '2021-02', '2021-03', '2021-04', '2021-05', '2021-06', '2021-07']\n",
        "\n",
        "file = []\n",
        "for filepath in glob.iglob(r'/content/gdrive/MyDrive/br_mc_auxilio_emergencial/input/*.csv'):\n",
        "    file.append(filepath)\n",
        "\n",
        "mes=[]\n",
        "for i in range(len(file)) : mes.append(int(file[i][56:62]))\n",
        "\n",
        "dict_mes = dict(zip(mes, file))\n",
        "dict_mes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WVsmZBVVTIB"
      },
      "source": [
        "for i in mes:\n",
        "  if i >= 202101:\n",
        "    df = pd.DataFrame()\n",
        "    df = pd.read_csv(dict_mes[i], sep=';', dtype='string', encoding='latin-1')\n",
        "    df.rename(columns=rename, inplace=True)\n",
        "    df['valor_beneficio'] = df['valor_beneficio'].apply(lambda x: str(x).replace(',','.'))\n",
        "    df.loc[(df['nis_beneficiario'] == '00000000000'), 'nis_beneficiario'] = ''\n",
        "    df.loc[(df['nis_responsavel'] == '-2'), 'nis_responsavel'] = ''\n",
        "    df['cpf_responsavel'] = df['cpf_responsavel'].fillna('')\n",
        "    df.loc[(df['nome_responsavel'] == 'Não se aplica'), 'nome_responsavel'] = ''\n",
        "    df.loc[(df['observacao'] == 'Não há'), 'observacao'] = ''\n",
        "    df['observacao'] = df['observacao'].fillna('')\n",
        "    df.sort_values(['nis_beneficiario', 'nis_responsavel', 'cpf_beneficiario', 'cpf_responsavel', 'parcela'], inplace=True)\n",
        "    df.drop_duplicates(subset=['nis_beneficiario', 'nis_responsavel', 'cpf_beneficiario', 'cpf_responsavel', 'parcela', 'observacao'], keep ='first', inplace=True)\n",
        "    df = df[ordem]\n",
        "    for uf in ufs:\n",
        "      for m in meses:\n",
        "        exec(\"df_{} = df[df['sigla_uf']== uf]\".format(uf))\n",
        "        exec(\"df_{}.drop(['sigla_uf', 'mes'], axis=1, inplace=True)\".format(uf))\n",
        "        print(\"Particionando {} de {}\".format(uf,m))\n",
        "        exec(\"df_{}.to_csv('/content/gdrive/MyDrive/br_mc_auxilio_emergencial/output/mes={}/sigla_uf={}/microdados2.csv',index=False, encoding='utf-8', na_rep='')\".format(uf,m,uf))\n",
        "    exec('del df_{}'.format(uf))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}