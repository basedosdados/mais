{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /home/tricktx/.pyenv/versions/3.10.9/envs/mais/lib/python3.10/site-packages (1.3.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " # ! Importando as bibliotecas\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import basedosdados as bd\n",
    "import numpy as np\n",
    "import unidecode\n",
    "from pathlib import Path\n",
    "\n",
    "# Visualizando todas as colunas do DataFrame. \n",
    "# Importante quando for trabalhar com muitas colunas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_row', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ! URL da página que contém os links de download\n",
    "url = \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/serie-historica-de-precos-de-combustiveis\"\n",
    "\n",
    "# ! Fazer solicitação GET para a página\n",
    "# ? O método get() retorna um objeto Response\n",
    "response = requests.get(url)\n",
    "\n",
    "# ! Analisar o HTML da página usando a biblioteca BeautifulSoup\n",
    "# ? Usando a biblioteca BeautifulSoup para analisar o HTML da página\n",
    "# * Primeiro argumento: o conteúdo HTML da página\n",
    "# * Segundo argumento: o parser HTML que será usado para analisar o HTML\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "\n",
    "# ! Encontrar todos os links de download para arquivos CSV\n",
    "# ? Usando o método find_all() para encontrar todos os elementos <a> com o atributo href\n",
    "# * Segundo argumento: Uma função lambda que é usada como filtro adicional para encontrar apenas os links que terminam com .csv\n",
    "links = soup.find_all(\"a\", href=lambda href: href and href.endswith(\".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "diretorio = \"input/\"\n",
    "\n",
    "if not os.path.exists(diretorio):\n",
    "    os.mkdir(diretorio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in links:\n",
    "    filename = link.get(\"href\").split(\"/\")[-1]\n",
    "    file_url = link.get(\"href\")\n",
    "    response = requests.get(file_url)\n",
    "\n",
    "    with open(f'input/{filename}', \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    print(f'Arquivo {filename} baixado com sucesso!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_partitions(data: pd.DataFrame, partition_columns: list[str], savepath: str):\n",
    "    \"\"\"Save data in to hive patitions schema, given a dataframe and a list of partition columns.\n",
    "    Args:\n",
    "        data (pandas.core.frame.DataFrame): Dataframe to be partitioned.\n",
    "        partition_columns (list): List of columns to be used as partitions.\n",
    "        savepath (str, pathlib.PosixPath): folder path to save the partitions\n",
    "    Exemple:\n",
    "        data = {\n",
    "            \"ano\": [2020, 2021, 2020, 2021, 2020, 2021, 2021,2025],\n",
    "            \"mes\": [1, 2, 3, 4, 5, 6, 6,9],\n",
    "            \"sigla_uf\": [\"SP\", \"SP\", \"RJ\", \"RJ\", \"PR\", \"PR\", \"PR\",\"PR\"],\n",
    "            \"dado\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\",'h'],\n",
    "        }\n",
    "        to_partitions(\n",
    "            data=pd.DataFrame(data),\n",
    "            partition_columns=['ano','mes','sigla_uf'],\n",
    "            savepath='partitions/'\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(data, (pd.core.frame.DataFrame)):\n",
    "\n",
    "        savepath = Path(savepath)\n",
    "\n",
    "        # create unique combinations between partition columns\n",
    "        unique_combinations = (\n",
    "            data[partition_columns]\n",
    "            .drop_duplicates(subset=partition_columns)\n",
    "            .to_dict(orient=\"records\")\n",
    "        )\n",
    "\n",
    "        for filter_combination in unique_combinations:\n",
    "            patitions_values = [\n",
    "                f\"{partition}={value}\"\n",
    "                for partition, value in filter_combination.items()\n",
    "            ]\n",
    "\n",
    "            # get filtered data\n",
    "            df_filter = data.loc[\n",
    "                data[filter_combination.keys()]\n",
    "                .isin(filter_combination.values())\n",
    "                .all(axis=1),\n",
    "                :,\n",
    "            ]\n",
    "            df_filter = df_filter.drop(columns=partition_columns)\n",
    "\n",
    "            # create folder tree\n",
    "            filter_save_path = Path(savepath / \"/\".join(patitions_values))\n",
    "            filter_save_path.mkdir(parents=True, exist_ok=True)\n",
    "            file_filter_save_path = Path(filter_save_path) / \"data.csv\"\n",
    "\n",
    "            # append data to csv\n",
    "            df_filter.to_csv(\n",
    "                file_filter_save_path,\n",
    "                index=False,\n",
    "                mode=\"a\",\n",
    "                header=not file_filter_save_path.exists(),\n",
    "            )\n",
    "    else:\n",
    "        raise BaseException(\"Data need to be a pandas DataFrame\")\n",
    "\n",
    "rename = {'Revenda': 'nome_estabelecimento', 'CNPJ da Revenda':'cnpj_revenda', 'Bairro':'bairro_revenda', 'Cep':'cep_revenda', 'Produto':'produto',\n",
    "          'Valor de Venda':'preco_venda', 'Valor de Compra':'preco_compra', 'Unidade de Medida':'unidade_medida', 'Bandeira':'bandeira_revenda', 'Estado - Sigla' : 'sigla_uf',\n",
    "          'Municipio':'nome', 'Data da Coleta': 'data_coleta', 'Nome da Rua':'nome_rua', 'Numero Rua':'numero_rua', 'Complemento':'complemento'}\n",
    "\n",
    "ordem = ['ano', 'sigla_uf', 'id_municipio', 'bairro_revenda', 'cep_revenda', 'endereco_revenda', 'cnpj_revenda', 'nome_estabelecimento', 'bandeira_revenda', \n",
    "         'data_coleta', 'produto', 'unidade_medida', 'preco_compra', 'preco_venda']\n",
    "\n",
    "# ! Carregando os dados direto do Diretório de municipio da BD\n",
    "# Para carregar o dado direto no pandas\n",
    "id_municipio = bd.read_table(dataset_id='br_bd_diretorios_brasil',\n",
    "table_id='municipio',\n",
    "billing_project_id=\"basedosdados-dev\") \n",
    "\n",
    "# ! Tratamento do id_municipio para mergear com a base\n",
    "id_municipio['nome'] = id_municipio['nome'].str.upper()\n",
    "id_municipio['nome'] = id_municipio['nome'].apply(unidecode.unidecode)\n",
    "id_municipio['nome'] = id_municipio['nome'].replace(\"ESPIGAO D'OESTE\", \"ESPIGAO DO OESTE\")\n",
    "id_municipio['nome'] = id_municipio['nome'].replace(\"SANT'ANA DO LIVRAMENTO\", \"SANTANA DO LIVRAMENTO\")\n",
    "id_municipio = id_municipio[['id_municipio', 'nome']]\n",
    "\n",
    "for a in {*range(2004, 2023)}:\n",
    "    df = pd.read_csv(f\"input/ca-{a}-01.csv\", sep=\";\", encoding=\"ISO-8859-1\") # ! combustiveis automativos 2004 - Primeiro semestre\n",
    "    dh = pd.read_csv(f\"input/glp-{a}-01.csv\", sep=\";\", encoding=\"ISO-8859-1\") # ! glp 2004 - Primeiro semestre\n",
    "    dk = pd.read_csv(f\"input/ca-{a}-02.csv\", sep=\";\", encoding=\"ISO-8859-1\") # ! combustiveis automativos 2004 - Primeiro semestre\n",
    "    dz = pd.read_csv(f\"input/glp-{a}-02.csv\", sep=\";\", encoding=\"ISO-8859-1\") # ! glp 2004 - Primeiro semestre\n",
    "    df_fx = df.append([dh, dk, dz], ignore_index=True)\n",
    "    \n",
    "    df_fx = pd.merge(id_municipio, df_fx, how='right', left_on=['nome'], right_on=['Municipio'])\n",
    "    \n",
    "    df_fx.rename(columns={'Municipio':'nome'}, inplace=True)\n",
    "    \n",
    "    #df_fx = pd.merge(id_cep, df_fx, how='right', on='cep_revenda') # Conversar depois com a Gabi sobre a coluna de CEP\n",
    "    df_fx['endereco_revenda'] = df_fx['Nome da Rua'].fillna('')  + ',' + ' ' + df_fx['Numero Rua'].fillna('') + ',' + ' ' + df_fx['Complemento'].fillna('')\n",
    "    \n",
    "    df_fx.rename(columns={'Data da Coleta':'data_coleta'}, inplace=True)\n",
    "    \n",
    "    df_fx = df_fx.query('data_coleta != \"08/\"')\n",
    "    \n",
    "    df_fx['data_coleta'] = pd.to_datetime(df_fx['data_coleta'], format='%d/%m/%Y')\n",
    "   \n",
    "    df_fx['Produto'] = df_fx['Produto'].str.lower()\n",
    "   \n",
    "    df_fx['ano'] = df_fx['data_coleta'].dt.year\n",
    "   \n",
    "    df_fx['ano'].replace('nan', '', inplace=True)\n",
    "   \n",
    "    df_fx.rename(columns=rename, inplace=True)\n",
    "  \n",
    "    df_fx = df_fx[ordem]\n",
    "   \n",
    "    df_fx['ano'] = df_fx['ano'].apply(lambda x: str(x).replace('.0', ''))\n",
    "   \n",
    "    df_fx['cep_revenda'] = df_fx['cep_revenda'].apply(lambda x: str(x).replace('-', ''))\n",
    "   \n",
    "    df_fx['unidade_medida'] = df_fx['unidade_medida'].map({'R$ / litro': 'R$/litro', 'R$ / m³':'R$/m3', 'R$ / 13 kg':'R$/13kg'})\n",
    "   \n",
    "    df_fx['nome_estabelecimento'] = df_fx['nome_estabelecimento'].apply(lambda x: str(x).replace(',',''))\n",
    "  \n",
    "    df_fx['preco_compra'] = df_fx['preco_compra'].apply(lambda x: str(x).replace(',','.'))\n",
    " \n",
    "    df_fx['preco_venda'] = df_fx['preco_venda'].apply(lambda x: str(x).replace(',','.'))\n",
    "  \n",
    "    df_fx['preco_venda'] = df_fx['preco_venda'].replace('nan', '')\n",
    "  \n",
    "    df_fx['preco_compra'] = df_fx['preco_compra'].replace('nan', '')\n",
    "  \n",
    "    df_fx.sort_values('data_coleta', inplace=True)\n",
    "   \n",
    "    df_fx.drop_duplicates()\n",
    "  \n",
    "\n",
    "    to_partitions(df_fx,\n",
    "    partition_columns=['ano'],\n",
    "    savepath = 'output/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mais",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
